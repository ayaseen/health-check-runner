= OpenShift Health Check Report

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Key

[cols="1,3", options=header]
|===
|Value
|Description

|
{set:cellbgcolor:#FF0000}
Changes Required
|
{set:cellbgcolor!}
Indicates Changes Required for system stability, subscription compliance, or other reason.

|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|
{set:cellbgcolor!}
Indicates Changes Recommended to align with recommended practices, but not urgently required

|
{set:cellbgcolor:#A6B9BF}
N/A
|
{set:cellbgcolor!}
No advise given on line item.  For line items which are data-only to provide context.

|
{set:cellbgcolor:#80E5FF}
Advisory
|
{set:cellbgcolor!}
No change required or recommended, but additional information provided.

|
{set:cellbgcolor:#00FF00}
No Change
|
{set:cellbgcolor!}
No change required. In alignment with recommended practices.

|
{set:cellbgcolor:#FFFFFF}
To Be Evaluated
|
{set:cellbgcolor!}
Not yet evaluated. Will appear only in draft copies.
|===

= Summary


[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-type.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Type>>

| Default OpenShift ingress controller is in use 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-placement.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Placement>>

| Ingress controller node placement is not configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-replica.item

// Category
|
{set:cellbgcolor!}
Network

// Item Evaluated
a|
<<Ingress Controller Replicas>>

| Ingress controller has insufficient replicas: 2 (recommended: >= 3) 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-ingress-certificate.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Default Ingress Certificate>>

| Custom certificate is properly configured for the default ingress controller 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cni-network-plugin.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<CNI Network Plugin>>

| Cluster is using the recommended CNI network plugin: OVNKubernetes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/network-policy.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Network Policy>>

| Found 6 network policies in the cluster 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller>>

| Ingress controller has 2 configuration issues 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-classes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Classes>>

| Default storage class 'gp3-csi' is configured and RWX-capable storage is available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/persistent-volumes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Persistent Volumes>>

| All 19 persistent volumes are healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-performance.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Performance>>

| No storage classes with explicit performance characteristics found 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-provider.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Provider>>

| Infrastructure provider type: AWS 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/installation-type.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Installation Type>>

| Installation type: IPI 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/upi-machinesets.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<UPI with MachineSets>>

| This is an IPI installation, UPI MachineSets check is not applicable 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-status.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Status>>

| All 6 nodes are ready 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-usage.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Usage>>

| All nodes are within resource usage thresholds 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-version.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Version>>

| Cluster version 4.16.38 is up to date 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-operators.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Operators>>

| All cluster operators are available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/control-node-schedulable.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Control Plane Node Schedulability>>

| All control plane nodes are properly configured to prevent regular workloads 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Nodes>>

| Found 3 infrastructure nodes but not all are properly tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/workload-off-infra-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Workloads on Infrastructure Nodes>>

| 10 user workloads are running on infrastructure nodes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-node-schedule.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Default Node Schedule>>

| No custom namespace node selectors are configured 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-machine-config-pool.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Machine Config Pool>>

| No dedicated infrastructure machine config pool found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubelet-garbage-collection.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Kubelet Garbage Collection>>

| No custom kubelet garbage collection configuration found (using defaults) 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-health.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Health>>

| ETCD cluster is healthy and performing well 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-performance.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Performance>>

| ETCD is performing optimally 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/proxy-settings.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<OpenShift Proxy Settings>>

| OpenShift Proxy is not configured 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-taints.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Taints>>

| 3 infrastructure nodes are not tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/application-probes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Application Probes>>

| Many user workloads are missing probes: 71.4% missing readiness probes, 42.9% missing liveness probes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/resource-quotas.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Resource Quotas>>

| Only 0.0% of user namespaces (0 out of 8) have both resource quotas and limit ranges configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/emptydir-volumes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<EmptyDir Volumes>>

| 66.7% of user workloads (6 out of 9) are using emptyDir volumes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/limit-range.item

// Category
|
{set:cellbgcolor!}
App Dev

// Item Evaluated
a|
<<LimitRange Configuration>>

| Only 12.5% of user namespaces (1 out of 8) have LimitRange configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-default-scc.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Security Context Constraint>>

| Default security context constraint (restricted) has not been modified 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-project-template.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Project Template>>

| Default project template is configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/self-provisioner.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Self Provisioner>>

| Self-provisioner role binding includes system:authenticated:oauth, allowing uncontrolled namespace creation 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubeadmin-user.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Kubeadmin User>>

| The kubeadmin user has been removed 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/identity-provider.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Identity Provider Configuration>>

| Identity providers are configured (HTPasswd), but no LDAP provider found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-backup.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Backup>>

| No CronJobs found that might be backing up etcd 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-encryption.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Encryption>>

| ETCD encryption is not enabled 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/elevated-privileges.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Elevated Privileges>>

| No user workloads using privileged containers were found 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-stack-config.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Stack Configuration>>

| Monitoring stack configuration needs improvement: monitoring stack is missing configuration for components: metricsServer, node placement configuration incomplete for components: metricsServer, remote write storage not configured for long-term metrics retention 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarders-ops.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Forwarders OPS>>

| No external log forwarding is configured for operations logs 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarder.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Log Forwarding>>

| No external log forwarding configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-install.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Installation>>

| OpenShift Logging with Loki is installed and configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-health.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Health>>

| Loki Logging is healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-placement.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Component Placement>>

| All Loki pods are scheduled on infrastructure nodes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Storage>>

| Loki storage schema needs to be updated 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/service-monitors.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Service Monitors>>

| Found 3 ServiceMonitors for application metrics monitoring 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/alerts-forwarding.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Alerts Forwarding>>

| No external alert forwarding configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Storage>>

| OpenShift monitoring components have persistent storage configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/user-workload-monitoring.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<User Workload Monitoring>>

| User Workload Monitoring is enabled but missing configuration for components: prometheusOperator, prometheus, thanosRuler, alertmanager 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

|===

<<<

{set:cellbgcolor!}

# Network

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-type.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Type>>

| Default OpenShift ingress controller is in use 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-placement.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Placement>>

| Ingress controller node placement is not configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-replica.item

// Category
|
{set:cellbgcolor!}
Network

// Item Evaluated
a|
<<Ingress Controller Replicas>>

| Ingress controller has insufficient replicas: 2 (recommended: >= 3) 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-ingress-certificate.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Default Ingress Certificate>>

| Custom certificate is properly configured for the default ingress controller 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cni-network-plugin.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<CNI Network Plugin>>

| Cluster is using the recommended CNI network plugin: OVNKubernetes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/network-policy.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Network Policy>>

| Found 6 network policies in the cluster 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller>>

| Ingress controller has 2 configuration issues 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Storage

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-classes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Classes>>

| Default storage class 'gp3-csi' is configured and RWX-capable storage is available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/persistent-volumes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Persistent Volumes>>

| All 19 persistent volumes are healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-performance.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Performance>>

| No storage classes with explicit performance characteristics found 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Cluster Config

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-provider.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Provider>>

| Infrastructure provider type: AWS 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/installation-type.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Installation Type>>

| Installation type: IPI 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/upi-machinesets.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<UPI with MachineSets>>

| This is an IPI installation, UPI MachineSets check is not applicable 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-status.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Status>>

| All 6 nodes are ready 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-usage.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Usage>>

| All nodes are within resource usage thresholds 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-version.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Version>>

| Cluster version 4.16.38 is up to date 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-operators.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Operators>>

| All cluster operators are available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/control-node-schedulable.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Control Plane Node Schedulability>>

| All control plane nodes are properly configured to prevent regular workloads 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Nodes>>

| Found 3 infrastructure nodes but not all are properly tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/workload-off-infra-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Workloads on Infrastructure Nodes>>

| 10 user workloads are running on infrastructure nodes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-node-schedule.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Default Node Schedule>>

| No custom namespace node selectors are configured 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-machine-config-pool.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Machine Config Pool>>

| No dedicated infrastructure machine config pool found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubelet-garbage-collection.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Kubelet Garbage Collection>>

| No custom kubelet garbage collection configuration found (using defaults) 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-health.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Health>>

| ETCD cluster is healthy and performing well 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-performance.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Performance>>

| ETCD is performing optimally 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/proxy-settings.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<OpenShift Proxy Settings>>

| OpenShift Proxy is not configured 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-taints.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Taints>>

| 3 infrastructure nodes are not tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# App Dev

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/application-probes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Application Probes>>

| Many user workloads are missing probes: 71.4% missing readiness probes, 42.9% missing liveness probes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/resource-quotas.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Resource Quotas>>

| Only 0.0% of user namespaces (0 out of 8) have both resource quotas and limit ranges configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/emptydir-volumes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<EmptyDir Volumes>>

| 66.7% of user workloads (6 out of 9) are using emptyDir volumes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/limit-range.item

// Category
|
{set:cellbgcolor!}
App Dev

// Item Evaluated
a|
<<LimitRange Configuration>>

| Only 12.5% of user namespaces (1 out of 8) have LimitRange configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Security

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-default-scc.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Security Context Constraint>>

| Default security context constraint (restricted) has not been modified 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-project-template.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Project Template>>

| Default project template is configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/self-provisioner.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Self Provisioner>>

| Self-provisioner role binding includes system:authenticated:oauth, allowing uncontrolled namespace creation 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubeadmin-user.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Kubeadmin User>>

| The kubeadmin user has been removed 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/identity-provider.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Identity Provider Configuration>>

| Identity providers are configured (HTPasswd), but no LDAP provider found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-backup.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Backup>>

| No CronJobs found that might be backing up etcd 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-encryption.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Encryption>>

| ETCD encryption is not enabled 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/elevated-privileges.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Elevated Privileges>>

| No user workloads using privileged containers were found 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Op-Ready

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-stack-config.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Stack Configuration>>

| Monitoring stack configuration needs improvement: monitoring stack is missing configuration for components: metricsServer, node placement configuration incomplete for components: metricsServer, remote write storage not configured for long-term metrics retention 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarders-ops.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Forwarders OPS>>

| No external log forwarding is configured for operations logs 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarder.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Log Forwarding>>

| No external log forwarding configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-install.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Installation>>

| OpenShift Logging with Loki is installed and configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-health.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Health>>

| Loki Logging is healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-placement.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Component Placement>>

| All Loki pods are scheduled on infrastructure nodes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Storage>>

| Loki storage schema needs to be updated 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/service-monitors.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Service Monitors>>

| Found 3 ServiceMonitors for application metrics monitoring 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/alerts-forwarding.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Alerts Forwarding>>

| No external alert forwarding configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Storage>>

| OpenShift monitoring components have persistent storage configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/user-workload-monitoring.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<User Workload Monitoring>>

| User Workload Monitoring is enabled but missing configuration for components: prometheusOperator, prometheus, thanosRuler, alertmanager 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

== Ingress Controller Type

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Ingress Controller Type Analysis ===

Ingress Controller Type: default

Ingress Controller Deployment:
[source, yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "2"
  creationTimestamp: "2025-04-14T17:56:22Z"
  generation: 2
  labels:
    ingresscontroller.operator.openshift.io/owning-ingresscontroller: default
  name: router-default
  namespace: openshift-ingress
  resourceVersion: "766755"
  uid: 9c98bfec-3be5-4479-947f-ea0d9cda693e
spec:
  minReadySeconds: 30
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 50%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
      creationTimestamp: null
      labels:
        ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
        ingresscontroller.operator.openshift.io/hash: 54975c748
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node.openshift.io/remote-worker
                operator: NotIn
                values:
                - ""
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: ingresscontroller.operator.openshift.io/deployment-ingresscontroller
                  operator: In
                  values:
                  - default
                - key: ingresscontroller.operator.openshift.io/hash
                  operator: NotIn
                  values:
                  - 54975c748
              topologyKey: kubernetes.io/hostname
            weight: 100
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: ingresscontroller.operator.openshift.io/deployment-ingresscontroller
                operator: In
                values:
                - default
              - key: ingresscontroller.operator.openshift.io/hash
                operator: In
                values:
                - 54975c748
            topologyKey: kubernetes.io/hostname
      containers:
      - env:
        - name: DEFAULT_CERTIFICATE_DIR
          value: /etc/pki/tls/private
        - name: DEFAULT_DESTINATION_CA_PATH
          value: /var/run/configmaps/service-ca/service-ca.crt
        - name: RELOAD_INTERVAL
          value: 5s
        - name: ROUTER_ALLOW_WILDCARD_ROUTES
          value: "false"
        - name: ROUTER_CANONICAL_HOSTNAME
          value: router-default.apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
        - name: ROUTER_CIPHERS
          value: ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
        - name: ROUTER_CIPHERSUITES
          value: TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
        - name: ROUTER_DISABLE_HTTP2
          value: "true"
        - name: ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK
          value: "false"
        - name: ROUTER_DOMAIN
          value: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
        - name: ROUTER_LOAD_BALANCE_ALGORITHM
          value: random
        - name: ROUTER_METRICS_TLS_CERT_FILE
          value: /etc/pki/tls/metrics-certs/tls.crt
        - name: ROUTER_METRICS_TLS_KEY_FILE
          value: /etc/pki/tls/metrics-certs/tls.key
        - name: ROUTER_METRICS_TYPE
          value: haproxy
        - name: ROUTER_SERVICE_NAME
          value: default
        - name: ROUTER_SERVICE_NAMESPACE
          value: openshift-ingress
        - name: ROUTER_SET_FORWARDED_HEADERS
          value: append
        - name: ROUTER_TCP_BALANCE_SCHEME
          value: source
        - name: ROUTER_THREADS
          value: "4"
        - name: ROUTER_USE_PROXY_PROTOCOL
          value: "true"
        - name: SSL_MIN_VERSION
          value: TLSv1.2
        - name: STATS_PASSWORD_FILE
          value: /var/lib/haproxy/conf/metrics-auth/statsPassword
        - name: STATS_PORT
          value: "1936"
        - name: STATS_USERNAME_FILE
          value: /var/lib/haproxy/conf/metrics-auth/statsUsername
        image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f34084a54f1d4029f418c87adf663ef823125afaded7705cdcc4f6f959275cf6
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 1936
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          terminationGracePeriodSeconds: 10
          timeoutSeconds: 1
        name: router
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 1936
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz/ready
            port: 1936
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
        securityContext:
          allowPrivilegeEscalation: true
          readOnlyRootFilesystem: false
        startupProbe:
          failureThreshold: 120
          httpGet:
            path: /healthz/ready
            port: 1936
            scheme: HTTP
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        - mountPath: /etc/pki/tls/private
          name: default-certificate
          readOnly: true
        - mountPath: /var/run/configmaps/service-ca
          name: service-ca-bundle
          readOnly: true
        - mountPath: /var/lib/haproxy/conf/metrics-auth
          name: stats-auth
          readOnly: true
        - mountPath: /etc/pki/tls/metrics-certs
          name: metrics-certs
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
        node-role.kubernetes.io/worker: ""
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: router
      serviceAccountName: router
      terminationGracePeriodSeconds: 3600
      topologySpreadConstraints:
      - labelSelector:
          matchExpressions:
          - key: ingresscontroller.operator.openshift.io/hash
            operator: In
            values:
            - 54975c748
        maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - name: default-certificate
        secret:
          defaultMode: 420
          secretName: cert-manager-ingress-cert
      - configMap:
          defaultMode: 420
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: service-ca-bundle
          optional: false
        name: service-ca-bundle
      - name: stats-auth
        secret:
          defaultMode: 420
          secretName: router-stats-default
      - name: metrics-certs
        secret:
          defaultMode: 420
          secretName: router-metrics-certs-default
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    lastUpdateTime: "2025-04-14T18:31:23Z"
    message: ReplicaSet "router-default-5fb6c748dd" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: "2025-04-16T12:55:41Z"
    lastUpdateTime: "2025-04-16T12:55:41Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 2
  readyReplicas: 2
  replicas: 2
  updatedReplicas: 2

----

Router Pods:
[source, bash]
----
NAME                              READY   STATUS    RESTARTS   AGE
router-default-5fb6c748dd-bgflq   1/1     Running   4          43h
router-default-5fb6c748dd-qf6jt   1/1     Running   4          43h

----

Available Ingress Controllers:
[source, bash]
----
NAME      AGE
default   44h

----

**Observation**

Default OpenShift ingress controller is in use

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Ingress Controller Placement

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Ingress Controller Placement Analysis ===

Ingress Controller Configuration:
[source, yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "766761"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

Node Selector Configuration: No information available

Router Pods:
[source, bash]
----
NAME                              READY   STATUS    RESTARTS   AGE   IP            NODE                                           NOMINATED NODE   READINESS GATES
router-default-5fb6c748dd-bgflq   1/1     Running   4          43h   10.131.0.19   ip-10-0-14-186.eu-central-1.compute.internal   <none>           <none>
router-default-5fb6c748dd-qf6jt   1/1     Running   4          43h   10.129.2.19   ip-10-0-20-141.eu-central-1.compute.internal   <none>           <none>

----

Infrastructure Nodes:
[source, bash]
----
NAME                                           STATUS   ROLES          AGE   VERSION
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker   43h   v1.29.14+7cf4c05

----

**Observation**

Ingress controller node placement is not configured

**Recommendation**

Configure the ingress controller to be placed on infrastructure nodes

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#nw-ingress-controller-configuration-parameters_configuring-ingress

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Ingress Controller Replicas

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Ingress Controller Replica Analysis ===

Ingress Controller Configuration:
[source, yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "766761"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

Router Pods:
[source, bash]
----
NAME                              READY   STATUS    RESTARTS   AGE
router-default-5fb6c748dd-bgflq   1/1     Running   4          43h
router-default-5fb6c748dd-qf6jt   1/1     Running   4          43h

----

Configured Replica Count: 2

Recommendation: Production environments should have at least 3 replicas for high availability.

**Observation**

Ingress controller has insufficient replicas: 2 (recommended: >= 3)

**Recommendation**

Increase the number of ingress controller replicas to at least 3 for high availability

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#configuring-ingress

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Ingress Certificate

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

Ingress Controller Configuration:
[source, yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "766761"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

Certificate Secret:
[source, yaml]
----
apiVersion: v1
data:
  tls.crt: -----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
MIIFgTCCBGmgAwIBAgIQOXJEOvkit1HX02wQ3TE1lTANBgkqhkiG9w0BAQwFADB7
MQswCQYDVQQGEwJHQjEbMBkGA1UECAwSR3JlYXRlciBNYW5jaGVzdGVyMRAwDgYD
VQQHDAdTYWxmb3JkMRowGAYDVQQKDBFDb21vZG8gQ0EgTGltaXRlZDEhMB8GA1UE
AwwYQUFBIENlcnRpZmljYXRlIFNlcnZpY2VzMB4XDTE5MDMxMjAwMDAwMFoXDTI4
MTIzMTIzNTk1OVowgYgxCzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpOZXcgSmVyc2V5
MRQwEgYDVQQHEwtKZXJzZXkgQ2l0eTEeMBwGA1UEChMVVGhlIFVTRVJUUlVTVCBO
ZXR3b3JrMS4wLAYDVQQDEyVVU0VSVHJ1c3QgUlNBIENlcnRpZmljYXRpb24gQXV0
aG9yaXR5MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAgBJlFzYOw9sI
s9CsVw127c0n00ytUINh4qogTQktZAnczomfzD2p7PbPwdzx07HWezcoEStH2jnG
vDoZtF+mvX2do2NCtnbyqTsrkfjib9DsFiCQCT7i6HTJGLSR1GJk23+jBvGIGGqQ
Ijy8/hPwhxR79uQfjtTkUcYRZ0YIUcuGFFQ/vDP+fmyc/xadGL1RjjWmp2bIcmfb
IWax1Jt4A8BQOujM8Ny8nkz+rwWWNR9XWrf/zvk9tyy29lTdyOcSOk2uTIq3XJq0
tyA9yn8iNK5+O2hmAUTnAU5GU5szYPeUvlM3kHND8zLDU+/bqv50TmnHa4xgk97E
xwzf4TKuzJM7UXiVZ4vuPVb+DNBpDxsP8yUmazNt925H+nND5X4OpWaxKXwyhGNV
icQNwZNUMBkTrNN9N6frXTpsNVzbQdcS2qlJC9/YgIoJk2KOtWbPJYjNhLixP6Q5
D9kCnusSTJV882sFqV4Wg8y4Z+LoE53MW4LTTLPtW//e5XOsIzstAL81VXQJSdhJ
WBp/kjbmUZIO8yZ9HE0XvMnsQybQv0FfQKlERPSZ51eHnlAfV1SoPv10Yy+xUGUJ
5lhCLkMaTLTwJUdZ+gQek9QmRkpQgbLevni3/GcV4clXhB4PY9bpYrrWX1Uu6lzG
KAgEJTm4Diup8kyXHAc/DVL17e8vgg8CAwEAAaOB8jCB7zAfBgNVHSMEGDAWgBSg
EQojPpbxB+zirynvgqV/0DCktDAdBgNVHQ4EFgQUU3m/WqorSs9UgOHYm8Cd8rID
ZsswDgYDVR0PAQH/BAQDAgGGMA8GA1UdEwEB/wQFMAMBAf8wEQYDVR0gBAowCDAG
BgRVHSAAMEMGA1UdHwQ8MDowOKA2oDSGMmh0dHA6Ly9jcmwuY29tb2RvY2EuY29t
L0FBQUNlcnRpZmljYXRlU2VydmljZXMuY3JsMDQGCCsGAQUFBwEBBCgwJjAkBggr
BgEFBQcwAYYYaHR0cDovL29jc3AuY29tb2RvY2EuY29tMA0GCSqGSIb3DQEBDAUA
A4IBAQAYh1HcdCE9nIrgJ7cz0C7M7PDmy14R3iJvm3WOnnL+5Nb+qh+cli3vA0p+
rvSNb3I8QzvAP+u431yqqcau8vzY7qN7Q/aGNnwU4M309z/+3ri0ivCRlv79Q2R+
/czSAaF9ffgZGclCKxO/WIu6pKJmBHaIkU4MiRTOok3JMrO66BQavHHxW/BBC5gA
CiIDEOUMsfnNkjcZ7Tvx5Dq2+UUTJnWvu6rvP3t3O9LEApE9GQDTF1w52z97GA1F
zZOFli9d31kWTz9RvdVFGD/tSo7oBmF0Ixa1DVBzJ0RHfxBdiSprhTEUxOipakyA
vGp4z7h/jnZymQyd/teRCBaho1+V
-----END CERTIFICATE-----

  tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBMXVNRGp6c2REMGdlU3RHbnJSbGc3ZTF2S0NxQkh3M3d5TFFycDFhUmFjZDlqUVVTClA3U3N1dmo3M2xGaCs3eU9tOThYMFRPKzNiYTdZN0taV3pVMXdqRW1lM1dGb1dqSDBCUUIybU5yaU81RUU1ZjIKVGtKc3Avd2ZLcy9uMlBlL05KK3VlRTAzOU1LZmJEUXI1YmFuR2luaHZOU2QxRm5CUnN2Q2JlUzZHYVNOeW9uUwpZL29TdlZja1cwK3RMbHA5Z3NjRC9EeWRPRHZ4RjE2dDFIc2U4UmZJbUVpcTdIOGptRVhpQkJmWDhmeEd6SjZNCk5JU0daalMzMjd0MVRPNU1ZSnJwdzErOWZPcHVUZ01hSFd2V2llbTBzNlVaRzVpVTRFbU1pYnBkeGd5b3NzbnYKTzNBdG8rZmRNT2xHY1E2Wmd3cHRGVmhRRk80d0ExYkl3NHc1MlFJREFRQUJBb0lCQURwWndxbXVINkg2cjE4Wgp0ekpsNWRBMTNTMUcvWEQxVkNEcjY3NnczeU9lTEJXUy92V1ZsaUhmWW1Wb0svY3pxSFhqQ3BDZ1FIL3NFczVyCnZRRHJYZ0d4WHdXL1Q5NFltY21DNVUzc0dWTkJmL0xpSE52N1pZM29CSDZYbmhsb2pOODVUU01WdE5BZDJUU0UKcGpibWRsSEFkYS9VL0ZhaFdMNWtyaE14UE9laEs3ZHhCQ2E3M1B4TEFGZk9oS3RRT21wd1RpQVUvZUFDbVJpWQpxc3FzKzRUeTU4dU5nUzRwL0o2OHNQL3p6aE5Za2ZiQWpWZ1FZNmZQZHNicS9CMlVFZHJKbnVRR2xXR0hPZmNECk9mTm12cUJJb2g3dFk5MGtRczZ0WVBnaEhJRUZoWE9haDBiR0NiL1lGU2JVbWwyREFnalZKRHRRRzc3VUx6YUIKTUtlNUIwRUNnWUVBMmdRVzhQWjdaNlhhQ0Z6NXB2b1hEcXdEc1FHTDhGWmpnZituWUt6Wi9oWDh1bjNkMEVubQpTZUJVRXptWUhtaXFsZHZWUUFITUV5eXdsNE1pUHZ5dVZDVUhvdi8vUjlNOHA5Q0tlb2hRTFJGYkZ2dDlDd2lpCjR1MWJoNytKUnVIdXRHT0xhWUdLYWJOOHQyY3Z3b2RseHhaZ0t4Mk5nSmxNM3dxVmxOcis4cjBDZ1lFQS9GTmIKTGZYN1hSOFZsWHJRU3NSR2txWUxUTUpleEN3Q1R0TGZJT3BMUHQxUW5YWEdHb0tzZlV3WUJOQVJucDZTTzcxQQpPOHBDTlJQVzVBSkMrTm9BRzNiWm4veFNPZ3gzenlXSUI4OFpjajdhcHBaRmwwWGh1cWNsUTE5c1dQbWJ0QlBjCnQvK1BqN2pzTXFLaEN3dlJyc0JYY050NkpzbTIzV1FCWDRMcEEwMENnWUVBMllOL0NBWkM5UlFweG5KTXVtV0cKSDZOTFE0eVJlOFlWSFp4ZCtEdzhlaDZodWNiZVNYT1JpVitCUW5VUjZqVHZEUGo1ZUJUUzhNYURvWWE1NGpIawpGQnVsMU9Db1o1YS9TNGJxd25uQjFGVWVsbjF2TWxUNWZvTGw4VGdOMUdTUjdmUHJJZWFXbkFMeGlXSXY2bHVBCkZmT2pHeUV2RG9Ea2l4dGFSZHVBeGRrQ2dZRUF0T3pRSkZTTXVHamhnUjFlM1QxL25RZVNjcWVzYTkxaWlrS0oKMjRBa0ZCOTgvbXJFY1ozaE1MYXZRa1F2WGgwMW5jQkZucG1ZZjVhS0wxT3YwZC8rMDdLVXVUS2pneHdOakdyTQo3MHlBWW5yUExVUHpBbFVKVmwwalN2dlJKTHRWK2ZtRCtpczFOVWpUbERzb1hsTkxOcEtVSUVnL2ZqQ0UvcG9HCm9nV3NMc0VDZ1lBSnYwYWdiRHFJNjhYUHJmaitNbmEzYVFHakVBVVlIM0lqR3k4ZW44b1g5ZTFHZFlmMTN0VUoKVWNJQkVmK3d1ZzlYZW10by96SFBET0Rwd09xT2NFQW4rRWkvMkNsZElPVFhrRjVybzBlN0xhbkFyZGFDazZaMgp1eE81ZnlMYlhEWks4MUFTdFlDa1lqRUhVRmdDWFloditFU1FrdjB3ckRqdU5CRFp3aEk1N3c9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=
kind: Secret
metadata:
  annotations:
    cert-manager.io/alt-names: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com,*.apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
    cert-manager.io/certificate-name: cert-manager-ingress-cert
    cert-manager.io/common-name: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
    cert-manager.io/ip-sans: ""
    cert-manager.io/issuer-group: cert-manager.io
    cert-manager.io/issuer-kind: ClusterIssuer
    cert-manager.io/issuer-name: zerossl-production-ec2
    cert-manager.io/uri-sans: ""
  creationTimestamp: "2025-04-14T18:30:44Z"
  labels:
    controller.cert-manager.io/fao: "true"
  name: cert-manager-ingress-cert
  namespace: openshift-ingress
  resourceVersion: "34434"
  uid: 129d605b-734f-4839-bc23-ab0346f02ed4
type: kubernetes.io/tls

----

**Observation**

Custom certificate is properly configured for the default ingress controller

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== CNI Network Plugin

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

Network Configuration:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: Network
  metadata:
    creationTimestamp: "2025-04-14T17:52:03Z"
    generation: 7
    name: cluster
    resourceVersion: "768344"
    uid: 96403a0a-3abf-47a0-a692-a3e5407a1195
  spec:
    clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
    externalIP:
      policy: {}
    networkDiagnostics:
      mode: ""
      sourcePlacement: {}
      targetPlacement: {}
    networkType: OVNKubernetes
    serviceNetwork:
    - 172.30.0.0/16
  status:
    clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
    clusterNetworkMTU: 8901
    conditions:
    - lastTransitionTime: "2025-04-16T12:57:35Z"
      message: ""
      reason: AsExpected
      status: "True"
      type: NetworkDiagnosticsAvailable
    networkType: OVNKubernetes
    serviceNetwork:
    - 172.30.0.0/16
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

Cluster is using the recommended CNI network plugin: OVNKubernetes

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Network Policy

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

Network Policies:
[source, bash]
----
NAMESPACE          NAME                                       POD-SELECTOR                                               AGE
openshift-gitops   openshift-gitops-redis-ha-network-policy   app.kubernetes.io/name=openshift-gitops-redis-ha-haproxy   43h
openshift-gitops   openshift-gitops-redis-network-policy      app.kubernetes.io/name=openshift-gitops-redis              43h
showroom-user1     allow-from-all-namespaces                  <none>                                                     43h
showroom-user1     allow-from-ingress-namespace               <none>                                                     43h
user1-argocd       user1-argo-redis-ha-network-policy         app.kubernetes.io/name=user1-argo-redis-ha-haproxy         43h
user1-argocd       user1-argo-redis-network-policy            app.kubernetes.io/name=user1-argo-redis                    43h

----

**Observation**

Found 6 network policies in the cluster

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Ingress Controller

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Comprehensive Ingress Controller Analysis ===

Ingress Controller Configuration:
[source, yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "766761"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-16T12:55:42Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

=== Individual Check Results ===

Type Check: Default OpenShift ingress controller is in use

Placement Check: Ingress controller node placement is not configured

Replica Check: Ingress controller has insufficient replicas: 2 (recommended: >= 3)

Certificate Check: Custom certificate is properly configured for the default ingress controller

Router Pods:
[source, bash]
----
NAME                              READY   STATUS    RESTARTS   AGE   IP            NODE                                           NOMINATED NODE   READINESS GATES
router-default-5fb6c748dd-bgflq   1/1     Running   4          43h   10.131.0.19   ip-10-0-14-186.eu-central-1.compute.internal   <none>           <none>
router-default-5fb6c748dd-qf6jt   1/1     Running   4          43h   10.129.2.19   ip-10-0-20-141.eu-central-1.compute.internal   <none>           <none>

----

=== Configuration Issues ===

- Ingress controller node placement is not configured
- Ingress controller has insufficient replicas: 2 (recommended: >= 3)

**Observation**

Ingress controller has 2 configuration issues

**Recommendation**

Configure the ingress controller to be placed on infrastructure nodes

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#nw-ingress-controller-configuration-parameters_configuring-ingress

Increase the number of ingress controller replicas to at least 3 for high availability

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#configuring-ingress

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Storage Classes

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Storage Classes Analysis ===

Storage Classes:
[source, bash]
----
NAME                          PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2-csi                       ebs.csi.aws.com                         Delete          WaitForFirstConsumer   true                   44h
gp3-csi (default)             ebs.csi.aws.com                         Delete          WaitForFirstConsumer   true                   44h
ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   43h
ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   43h
openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         Delete          Immediate              false                  43h

----

Storage Classes (YAML):
[source, yaml]
----
apiVersion: v1
items:
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    creationTimestamp: "2025-04-14T17:56:07Z"
    name: gp2-csi
    resourceVersion: "5893"
    uid: 0f5e301f-5847-4885-b03c-e88301aa68a1
  parameters:
    encrypted: "true"
    type: gp2
  provisioner: ebs.csi.aws.com
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      storageclass.kubernetes.io/is-default-class: "true"
    creationTimestamp: "2025-04-14T17:56:07Z"
    name: gp3-csi
    resourceVersion: "5888"
    uid: 128fdee7-f25c-4de2-b6a7-d6e7d1772021
  parameters:
    encrypted: "true"
    type: gp3
  provisioner: ebs.csi.aws.com
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides RWO Filesystem volumes, and RWO and RWX Block volumes
    creationTimestamp: "2025-04-14T18:44:57Z"
    name: ocs-storagecluster-ceph-rbd
    resourceVersion: "50068"
    uid: eb032e4e-ee68-4148-bd84-1e6b1cfdb4d4
  parameters:
    clusterID: openshift-storage
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
    csi.storage.k8s.io/fstype: ext4
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
    imageFeatures: layering,deep-flatten,exclusive-lock,object-map,fast-diff
    imageFormat: "2"
    pool: ocs-storagecluster-cephblockpool
  provisioner: openshift-storage.rbd.csi.ceph.com
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides RWO and RWX Filesystem volumes
    creationTimestamp: "2025-04-14T18:45:13Z"
    name: ocs-storagecluster-cephfs
    resourceVersion: "50388"
    uid: 3763c8da-be52-44ba-a936-536f75d43c70
  parameters:
    clusterID: openshift-storage
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
    fsName: ocs-storagecluster-cephfilesystem
  provisioner: openshift-storage.cephfs.csi.ceph.com
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
- apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides Object Bucket Claims (OBCs)
    creationTimestamp: "2025-04-14T18:46:07Z"
    name: openshift-storage.noobaa.io
    resourceVersion: "51326"
    uid: 7bd0b503-458c-472c-afd7-748b4a27e3e6
  parameters:
    bucketclass: noobaa-default-bucket-class
  provisioner: openshift-storage.noobaa.io/obc
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
kind: List
metadata:
  resourceVersion: ""

----

=== Storage Class Analysis ===

Default Storage Class: gp3-csi

Storage Class Names:
- gp2-csi
- gp3-csi
- ocs-storagecluster-ceph-rbd
- ocs-storagecluster-cephfs
- openshift-storage.noobaa.io

ReadWriteMany (RWX) Capable Storage: Available

**Observation**

Default storage class 'gp3-csi' is configured and RWX-capable storage is available

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Persistent Volumes

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Persistent Volumes Analysis ===

Persistent Volumes Overview:
[source, bash]
----
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                           STORAGECLASS                  VOLUMEATTRIBUTESCLASS   REASON   AGE   VOLUMEMODE
pvc-042f592c-6344-4c51-a130-d9a15d103e7b   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-gp3-csi-2-data-0rgwwv           gp3-csi                       <unset>                          43h   Block
pvc-0b70e9cf-722e-4055-bf1a-7b178597480d   10Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-ingester-0                  gp3-csi                       <unset>                          43h   Filesystem
pvc-0e7468f7-9013-4781-a378-e7504c88de8b   10Gi       RWO            Delete           Bound    openshift-monitoring/alertmanager-main-db-alertmanager-main-1   gp3-csi                       <unset>                          27h   Filesystem
pvc-15046389-45e4-43ef-a934-6d8191aa4ac0   50Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-c                               gp3-csi                       <unset>                          43h   Filesystem
pvc-312588a4-bb36-4ce8-b0ce-f048944c1606   50Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-index-gateway-1             gp3-csi                       <unset>                          43h   Filesystem
pvc-3eb073c6-ff9a-4b65-abff-6eb3acc2431c   50Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-a                               gp3-csi                       <unset>                          43h   Filesystem
pvc-4cf63059-2db0-4c4b-9988-32b928c9edcc   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-gp3-csi-0-data-0k2v72           gp3-csi                       <unset>                          43h   Block
pvc-50ef79db-6124-485a-b50d-591930fb8a4f   10Gi       RWO            Delete           Bound    openshift-monitoring/prometheus-k8s-db-prometheus-k8s-1         gp3-csi                       <unset>                          27h   Filesystem
pvc-54d7c6e5-8f03-4c33-8c3b-7946ebe63cda   10Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-ingester-1                  gp3-csi                       <unset>                          43h   Filesystem
pvc-56728660-2fd0-41e0-8f99-04f21131d0fb   50Gi       RWO            Delete           Bound    openshift-storage/db-noobaa-db-pg-0                             ocs-storagecluster-ceph-rbd   <unset>                          43h   Filesystem
pvc-5891c087-769f-426f-a925-be04857ef3eb   10Gi       RWO            Delete           Bound    openshift-monitoring/alertmanager-main-db-alertmanager-main-0   gp3-csi                       <unset>                          27h   Filesystem
pvc-6cc8583c-16ad-4301-9fca-8585886f4a06   10Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-compactor-0                 gp3-csi                       <unset>                          43h   Filesystem
pvc-87b3816d-4712-4e21-915e-229ab4c35a6d   150Gi      RWO            Delete           Bound    openshift-logging/wal-lokistack-ingester-1                      gp3-csi                       <unset>                          43h   Filesystem
pvc-93951a27-b38b-48ef-9f77-27d1d2ae3ba0   50Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-index-gateway-0             gp3-csi                       <unset>                          43h   Filesystem
pvc-a34c3fdb-8967-441e-ac6e-3a50559c074b   10Gi       RWO            Delete           Bound    openshift-monitoring/prometheus-k8s-db-prometheus-k8s-0         gp3-csi                       <unset>                          27h   Filesystem
pvc-b4bf6f83-f98e-4d29-ad8b-c9363e0e0830   5Gi        RWO            Delete           Bound    showroom-user1/showroom-terminal-lab-user-home                  gp3-csi                       <unset>                          43h   Filesystem
pvc-dad670ba-3445-46e2-b45d-11a336509baf   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-gp3-csi-1-data-0jmbq7           gp3-csi                       <unset>                          43h   Block
pvc-e1b9e364-f77d-43d5-9e22-df1a8a4063c4   50Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-b                               gp3-csi                       <unset>                          43h   Filesystem
pvc-e29df30a-e702-4502-b50f-364b9c98d3b2   150Gi      RWO            Delete           Bound    openshift-logging/wal-lokistack-ingester-0                      gp3-csi                       <unset>                          43h   Filesystem

----

=== Volume Status Analysis ===

Total Persistent Volumes: 19

Failed Persistent Volumes: None

Pending Persistent Volumes: None

Released Persistent Volumes: None

**Observation**

All 19 persistent volumes are healthy

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Storage Performance

[cols="^"] 
|===
|
{set:cellbgcolor:#80E5FF}
Advisory
|===

=== Storage Performance Analysis ===

Storage Classes Configuration:
[source, yaml]
----
apiVersion: v1
items:
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    creationTimestamp: "2025-04-14T17:56:07Z"
    name: gp2-csi
    resourceVersion: "5893"
    uid: 0f5e301f-5847-4885-b03c-e88301aa68a1
  parameters:
    encrypted: "true"
    type: gp2
  provisioner: ebs.csi.aws.com
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      storageclass.kubernetes.io/is-default-class: "true"
    creationTimestamp: "2025-04-14T17:56:07Z"
    name: gp3-csi
    resourceVersion: "5888"
    uid: 128fdee7-f25c-4de2-b6a7-d6e7d1772021
  parameters:
    encrypted: "true"
    type: gp3
  provisioner: ebs.csi.aws.com
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides RWO Filesystem volumes, and RWO and RWX Block volumes
    creationTimestamp: "2025-04-14T18:44:57Z"
    name: ocs-storagecluster-ceph-rbd
    resourceVersion: "50068"
    uid: eb032e4e-ee68-4148-bd84-1e6b1cfdb4d4
  parameters:
    clusterID: openshift-storage
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
    csi.storage.k8s.io/fstype: ext4
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
    imageFeatures: layering,deep-flatten,exclusive-lock,object-map,fast-diff
    imageFormat: "2"
    pool: ocs-storagecluster-cephblockpool
  provisioner: openshift-storage.rbd.csi.ceph.com
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides RWO and RWX Filesystem volumes
    creationTimestamp: "2025-04-14T18:45:13Z"
    name: ocs-storagecluster-cephfs
    resourceVersion: "50388"
    uid: 3763c8da-be52-44ba-a936-536f75d43c70
  parameters:
    clusterID: openshift-storage
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
    fsName: ocs-storagecluster-cephfilesystem
  provisioner: openshift-storage.cephfs.csi.ceph.com
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
- apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides Object Bucket Claims (OBCs)
    creationTimestamp: "2025-04-14T18:46:07Z"
    name: openshift-storage.noobaa.io
    resourceVersion: "51326"
    uid: 7bd0b503-458c-472c-afd7-748b4a27e3e6
  parameters:
    bucketclass: noobaa-default-bucket-class
  provisioner: openshift-storage.noobaa.io/obc
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
kind: List
metadata:
  resourceVersion: ""

----

Storage Classes Summary:
[source, bash]
----
NAME                          PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2-csi                       ebs.csi.aws.com                         Delete          WaitForFirstConsumer   true                   44h
gp3-csi (default)             ebs.csi.aws.com                         Delete          WaitForFirstConsumer   true                   44h
ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   43h
ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   43h
openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         Delete          Immediate              false                  43h

----

=== Performance Analysis ===

No explicit performance-related annotations found in storage classes.

Performance-related annotations might include:
- 'performance' - General performance tier indicators
- 'iops' - Input/Output Operations Per Second guarantees
- 'throughput' - Data transfer rate guarantees

=== Storage Performance Best Practices ===

1. Define different storage classes for different performance needs
2. Consider creating tiers like 'high', 'medium', and 'standard'
3. Label storage classes with performance characteristics
4. Document IOPS and throughput expectations for each class
5. Monitor storage performance regularly

**Observation**

No storage classes with explicit performance characteristics found

**Recommendation**

Consider defining storage classes with different performance tiers

Label storage classes with performance characteristics for better workload placement

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Provider

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Infrastructure Provider Information ===

Infrastructure Name: cluster-smd5h-x4xzc
Platform Type: AWS

Provider Details:
AWS Region: eu-central-1

Topology Information:
- Control Plane Topology: HighlyAvailable
- Infrastructure Topology: HighlyAvailable

Infrastructure Configuration:
[source, yaml]
----
apiVersion: config.openshift.io/v1
kind: Infrastructure
metadata:
  creationTimestamp: "2025-04-14T17:52:02Z"
  generation: 1
  name: cluster
  resourceVersion: "434"
  uid: d1c4bffc-05bb-4420-9e23-d853d09cade8
spec:
  cloudConfig:
    key: config
    name: cloud-provider-config
  platformSpec:
    aws: {}
    type: AWS
status:
  apiServerInternalURI: https://api-int.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  apiServerURL: https://api.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  controlPlaneTopology: HighlyAvailable
  cpuPartitioning: None
  etcdDiscoveryDomain: ""
  infrastructureName: cluster-smd5h-x4xzc
  infrastructureTopology: HighlyAvailable
  platform: AWS
  platformStatus:
    aws:
      region: eu-central-1
    type: AWS

----

**Observation**

Infrastructure provider type: AWS

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Installation Type

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Installation Type Analysis ===

Installation Type: Installer-Provisioned Infrastructure (IPI)
Description: This is an IPI installation where the OpenShift installer provisioned the infrastructure automatically.

Infrastructure Information:
- Infrastructure Name: cluster-smd5h-x4xzc
- Platform Type: AWS

Topology Information:
- Control Plane Topology: HighlyAvailable
- Infrastructure Topology: HighlyAvailable

Installation Indicators:
OpenShift Install ConfigMap:
[source, bash]
----
NAME                DATA   AGE
openshift-install   2      44h

----

MachineSets Information:
[source, bash]
----
NAME                                       DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-smd5h-x4xzc-worker-eu-central-1a   3         3         3       3           44h

----

Infrastructure Configuration:
[source, yaml]
----
apiVersion: config.openshift.io/v1
kind: Infrastructure
metadata:
  creationTimestamp: "2025-04-14T17:52:02Z"
  generation: 1
  name: cluster
  resourceVersion: "434"
  uid: d1c4bffc-05bb-4420-9e23-d853d09cade8
spec:
  cloudConfig:
    key: config
    name: cloud-provider-config
  platformSpec:
    aws: {}
    type: AWS
status:
  apiServerInternalURI: https://api-int.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  apiServerURL: https://api.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  controlPlaneTopology: HighlyAvailable
  cpuPartitioning: None
  etcdDiscoveryDomain: ""
  infrastructureName: cluster-smd5h-x4xzc
  infrastructureTopology: HighlyAvailable
  platform: AWS
  platformStatus:
    aws:
      region: eu-central-1
    type: AWS

----

**Observation**

Installation type: IPI

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== UPI with MachineSets

[cols="^"] 
|===
|
{set:cellbgcolor:#A6B9BF}
Not Applicable
|===

**Observation**

This is an IPI installation, UPI MachineSets check is not applicable

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Node Status

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Node Status Analysis ===

Node Overview:
[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   44h   v1.29.14+7cf4c05
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   44h   v1.29.14+7cf4c05
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   44h   v1.29.14+7cf4c05
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker           43h   v1.29.14+7cf4c05

----

Detailed Node Information:
[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.2.3      <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.31.43    <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.58.101   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

=== Node Status Summary ===

Total Nodes: 6
Ready Nodes: 6
Not Ready Nodes: 0

**Observation**

All 6 nodes are ready

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Node Usage

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Node Resource Usage Information ===

Node Resource Usage:
[source, bash]
----
NAME                                           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-10-0-14-186.eu-central-1.compute.internal   620m         4%     8973Mi          14%       
ip-10-0-2-3.eu-central-1.compute.internal      522m         6%     7469Mi          51%       
ip-10-0-20-141.eu-central-1.compute.internal   607m         3%     8173Mi          13%       
ip-10-0-31-43.eu-central-1.compute.internal    335m         4%     8324Mi          57%       
ip-10-0-58-101.eu-central-1.compute.internal   496m         6%     10516Mi         72%       
ip-10-0-8-87.eu-central-1.compute.internal     267m         1%     4531Mi          7%        

----

=== Node Resource Analysis ===


Node Information:
[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.2.3      <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.31.43    <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.58.101   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

**Observation**

All nodes are within resource usage thresholds

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Cluster Version

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Cluster Version Analysis ===

Current Version: 4.16.38
Latest Available Version: 4.14.0

Cluster Version Details:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: ClusterVersion
  metadata:
    creationTimestamp: "2025-04-14T17:52:08Z"
    generation: 2
    name: version
    resourceVersion: "768014"
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  spec:
    channel: stable-4.16
    clusterID: 9c3defd1-585f-489f-9dbc-3995377cf250
  status:
    availableUpdates: null
    capabilities:
      enabledCapabilities:
      - Build
      - CSISnapshot
      - CloudControllerManager
      - CloudCredential
      - Console
      - DeploymentConfig
      - ImageRegistry
      - Ingress
      - Insights
      - MachineAPI
      - NodeTuning
      - OperatorLifecycleManager
      - Storage
      - baremetal
      - marketplace
      - openshift-samples
      knownCapabilities:
      - Build
      - CSISnapshot
      - CloudControllerManager
      - CloudCredential
      - Console
      - DeploymentConfig
      - ImageRegistry
      - Ingress
      - Insights
      - MachineAPI
      - NodeTuning
      - OperatorLifecycleManager
      - Storage
      - baremetal
      - marketplace
      - openshift-samples
    conditions:
    - lastTransitionTime: "2025-04-14T17:52:30Z"
      message: Capabilities match configured spec
      reason: AsExpected
      status: "False"
      type: ImplicitlyEnabledCapabilities
    - lastTransitionTime: "2025-04-14T17:52:30Z"
      message: Payload loaded version="4.16.38" image="quay.io/openshift-release-dev/ocp-release@sha256:6da09834a9e0e30a79f77c13c2520a25172d8be3fc044dc2ad1392d69b2edfbf"
        architecture="amd64"
      reason: PayloadLoaded
      status: "True"
      type: ReleaseAccepted
    - lastTransitionTime: "2025-04-14T18:13:18Z"
      message: Done applying 4.16.38
      status: "True"
      type: Available
    - lastTransitionTime: "2025-04-15T13:10:35Z"
      status: "False"
      type: Failing
    - lastTransitionTime: "2025-04-14T18:13:18Z"
      message: Cluster version is 4.16.38
      status: "False"
      type: Progressing
    - lastTransitionTime: "2025-04-14T17:52:30Z"
      status: "True"
      type: RetrievedUpdates
    desired:
      channels:
      - candidate-4.16
      - candidate-4.17
      - candidate-4.18
      - eus-4.16
      - eus-4.18
      - fast-4.16
      - fast-4.17
      - fast-4.18
      - stable-4.16
      - stable-4.17
      - stable-4.18
      image: quay.io/openshift-release-dev/ocp-release@sha256:6da09834a9e0e30a79f77c13c2520a25172d8be3fc044dc2ad1392d69b2edfbf
      url: https://access.redhat.com/errata/RHSA-2025:3301
      version: 4.16.38
    history:
    - completionTime: "2025-04-14T18:13:18Z"
      image: quay.io/openshift-release-dev/ocp-release@sha256:6da09834a9e0e30a79f77c13c2520a25172d8be3fc044dc2ad1392d69b2edfbf
      startedTime: "2025-04-14T17:52:30Z"
      state: Completed
      verified: false
      version: 4.16.38
    observedGeneration: 2
    versionHash: FzelQgAxbzc=
kind: List
metadata:
  resourceVersion: ""

----

Update History:
[source, json]
----
[{"completionTime":"2025-04-14T18:13:18Z","image":"quay.io/openshift-release-dev/ocp-release@sha256:6da09834a9e0e30a79f77c13c2520a25172d8be3fc044dc2ad1392d69b2edfbf","startedTime":"2025-04-14T17:52:30Z","state":"Completed","verified":false,"version":"4.16.38"}]
----

=== Version Comparison ===

The cluster version 4.16.38 is newer than our reference version 4.14.0.

**Observation**

Cluster version 4.16.38 is up to date

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Cluster Operators

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Cluster Operators Status ===

Cluster Operators Overview:
[source, bash]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
authentication                             4.16.38   True        False         False      10h     
baremetal                                  4.16.38   True        False         False      44h     
cloud-controller-manager                   4.16.38   True        False         False      44h     
cloud-credential                           4.16.38   True        False         False      44h     
cluster-autoscaler                         4.16.38   True        False         False      44h     
config-operator                            4.16.38   True        False         False      44h     
console                                    4.16.38   True        False         False      43h     
control-plane-machine-set                  4.16.38   True        False         False      43h     
csi-snapshot-controller                    4.16.38   True        False         False      33h     
dns                                        4.16.38   True        False         False      24h     
etcd                                       4.16.38   True        False         False      44h     
image-registry                             4.16.38   True        False         False      43h     
ingress                                    4.16.38   True        False         False      64m     
insights                                   4.16.38   True        False         False      43h     
kube-apiserver                             4.16.38   True        False         False      43h     
kube-controller-manager                    4.16.38   True        False         False      44h     
kube-scheduler                             4.16.38   True        False         False      44h     
kube-storage-version-migrator              4.16.38   True        False         False      44h     
machine-api                                4.16.38   True        False         False      43h     
machine-approver                           4.16.38   True        False         False      44h     
machine-config                             4.16.38   True        False         False      44h     
marketplace                                4.16.38   True        False         False      44h     
monitoring                                 4.16.38   True        False         False      43h     
network                                    4.16.38   True        False         False      44h     
node-tuning                                4.16.38   True        False         False      43h     
openshift-apiserver                        4.16.38   True        False         False      64m     
openshift-controller-manager               4.16.38   True        False         False      24h     
openshift-samples                          4.16.38   True        False         False      43h     
operator-lifecycle-manager                 4.16.38   True        False         False      44h     
operator-lifecycle-manager-catalog         4.16.38   True        False         False      44h     
operator-lifecycle-manager-packageserver   4.16.38   True        False         False      33h     
service-ca                                 4.16.38   True        False         False      44h     
storage                                    4.16.38   True        False         False      33h     

----

=== Operator Analysis ===

Total Cluster Operators: 33

All operators are available.

**Observation**

All cluster operators are available

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Control Plane Node Schedulability

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Control Plane Node Schedulability Analysis ===

Control Plane Nodes:
[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.2.3      <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.31.43    <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.58.101   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

Taint Information:
[source, yaml]
----
ip-10-0-2-3.eu-central-1.compute.internal: [{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}]
ip-10-0-31-43.eu-central-1.compute.internal: [{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}]
ip-10-0-58-101.eu-central-1.compute.internal: [{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}]

----

=== Schedulability Summary ===

Total Control Plane Nodes: 3
Schedulable Control Plane Nodes: 0

=== Best Practices ===

Control plane nodes should be dedicated to control plane components to ensure stability and performance of the Kubernetes control plane.

To prevent scheduling of regular workloads on control plane nodes, either:
1. Add the NoSchedule taint: 'node-role.kubernetes.io/master=:NoSchedule'
2. Mark the node as unschedulable using 'oc adm cordon <node-name>'

This ensures that only pods with matching tolerations (typically control plane components) will be scheduled on these nodes.

**Observation**

All control plane nodes are properly configured to prevent regular workloads

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Nodes

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

Infrastructure Nodes:
[source, bash]
----
NAME                                           STATUS   ROLES          AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

**Observation**

Found 3 infrastructure nodes but not all are properly tainted

**Recommendation**

Add appropriate taints to infrastructure nodes to prevent regular workloads from being scheduled on them

Use 'oc adm taint nodes <node-name> node-role.kubernetes.io/infra=:NoSchedule'

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Workloads on Infrastructure Nodes

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Workload Placement Analysis ===

Infrastructure Nodes:
[source, bash]
----
NAME                                           STATUS   ROLES          AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

Total Infrastructure Nodes: 3
Total User Namespaces: 9

=== User Workloads on Infrastructure Nodes ===

Found 10 Application/User workloads running on infrastructure nodes:

- Pod 'cert-manager-c965b6b46-qjhgn' in namespace 'cert-manager' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'cert-manager-cainjector-7b97bcbf6-8slg5' in namespace 'cert-manager' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'cert-manager-webhook-6f8fdf478f-gvcmp' in namespace 'cert-manager' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'cert-manager-operator-controller-manager-79d54c6cf4-474d5' in namespace 'cert-manager-operator' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'showroom-775d8f49dc-4c2kk' in namespace 'showroom-user1' is running on infrastructure node 'ip-10-0-8-87.eu-central-1.compute.internal'
- Pod 'user1-argo-application-controller-0' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'user1-argo-dex-server-64bcdc785b-pwb8r' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-14-186.eu-central-1.compute.internal'
- Pod 'user1-argo-redis-fb879dcd7-9hc5k' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'user1-argo-repo-server-55c4b88b58-njk4d' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'user1-argo-server-69f4cbdd44-jcl6p' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-14-186.eu-central-1.compute.internal'

=== Best Practices ===

Infrastructure nodes should be dedicated to infrastructure components such as:
- Registry
- Router
- Monitoring
- Logging
- Metrics

User workloads should be scheduled on worker nodes to ensure proper resource allocation and prevent interference with critical infrastructure components.

**Observation**

10 user workloads are running on infrastructure nodes

**Recommendation**

Infrastructure nodes should be dedicated to infrastructure components

Add taints to infrastructure nodes to prevent user workloads from running on them

Consider moving these workloads to worker nodes

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Node Schedule

[cols="^"] 
|===
|
{set:cellbgcolor:#80E5FF}
Advisory
|===

=== Node Scheduling Configuration Analysis ===

Cluster Nodes:
[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.2.3      <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.31.43    <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   44h   v1.29.14+7cf4c05   10.0.58.101   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker           43h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

=== Node Role Distribution ===

Total Nodes: 6
Control Plane Nodes: 3
Infrastructure Nodes: 3
Worker Nodes: 3
Nodes Without Role: 0

Namespace Node Selectors:
[source, yaml]
----
cert-manager: 
cert-manager-operator: 
default: 
kube-node-lease: 
kube-public: 
kube-system: 
openshift: 
openshift-apiserver: 
openshift-apiserver-operator: 
openshift-authentication: 
openshift-authentication-operator: 
openshift-cloud-controller-manager: 
openshift-cloud-controller-manager-operator: 
openshift-cloud-credential-operator: 
openshift-cloud-network-config-controller: 
openshift-cloud-platform-infra: 
openshift-cluster-csi-drivers: 
openshift-cluster-machine-approver: 
openshift-cluster-node-tuning-operator: 
openshift-cluster-samples-operator: 
openshift-cluster-storage-operator: 
openshift-cluster-version: 
openshift-config: 
openshift-config-managed: 
openshift-config-operator: 
openshift-console: 
openshift-console-operator: 
openshift-console-user-settings: 
openshift-controller-manager: 
openshift-controller-manager-operator: 
openshift-dns: 
openshift-dns-operator: 
openshift-etcd: 
openshift-etcd-operator: 
openshift-gitops: 
openshift-host-network: 
openshift-image-registry: 
openshift-infra: 
openshift-ingress: 
openshift-ingress-canary: 
openshift-ingress-operator: 
openshift-insights: 
openshift-kni-infra: 
openshift-kube-apiserver: 
openshift-kube-apiserver-operator: 
openshift-kube-controller-manager: 
openshift-kube-controller-manager-operator: 
openshift-kube-scheduler: 
openshift-kube-scheduler-operator: 
openshift-kube-storage-version-migrator: 
openshift-kube-storage-version-migrator-operator: 
openshift-logging: 
openshift-machine-api: 
openshift-machine-config-operator: 
openshift-marketplace: 
openshift-monitoring: 
openshift-multus: 
openshift-network-diagnostics: 
openshift-network-node-identity: 
openshift-network-operator: 
openshift-node: 
openshift-nutanix-infra: 
openshift-oauth-apiserver: 
openshift-openstack-infra: 
openshift-operator-lifecycle-manager: 
openshift-operators: 
openshift-operators-redhat: 
openshift-ovirt-infra: 
openshift-ovn-kubernetes: 
openshift-route-controller-manager: 
openshift-service-ca: 
openshift-service-ca-operator: 
openshift-storage: 
openshift-user-workload-monitoring: 
openshift-vsphere-infra: 
showroom-user1: 
user1-argocd: 
user1-bgd: 
user1-bgdh: 
user1-bgdk: 
user1-todo: 

----

Scheduler Configuration: Using default configuration

=== Best Practices ===

1. All nodes should have appropriate role labels
2. Configure namespace node selectors to control workload placement
3. Use custom scheduler configuration for fine-grained control
4. Separate workloads, infrastructure, and control plane functions

**Observation**

No custom namespace node selectors are configured

**Recommendation**

Consider configuring namespace node selectors to control workload placement

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/nodes/index#nodes-scheduler-node-selectors

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Machine Config Pool

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

Machine Config Pools:
[source, bash]
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-f27053fdb325fead0ea67a44d88cc37f   True      False      False      3              3                   3                     0                      44h
worker   rendered-worker-a5adabfd0c4b230acaad384a895530fd   True      False      False      3              3                   3                     0                      44h

----

**Observation**

No dedicated infrastructure machine config pool found

**Recommendation**

Create a dedicated infrastructure machine config pool

In a production deployment, it is recommended that you deploy at least three machine sets to hold infrastructure components

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/machine_management/index#creating-infrastructure-machinesets

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Kubelet Garbage Collection

[cols="^"] 
|===
|
{set:cellbgcolor:#80E5FF}
Advisory
|===

=== Kubelet Garbage Collection Analysis ===

Kubelet Configuration:
[source, yaml]
----
No custom kubelet configuration found
----

Machine Config Pools:
[source, text]
----
master worker
----

Node Storage Issues: None detected

=== Configuration Status ===

Custom Kubelet Configuration Exists: false
Garbage Collection Thresholds Configured: false
Image Garbage Collection Configured: false
Container Log Size Limits Configured: false
Node Storage Issues Detected: false

**Observation**

No custom kubelet garbage collection configuration found (using defaults)

**Recommendation**

Consider configuring kubelet garbage collection parameters for production environments

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/nodes/index#nodes-nodes-garbage-collection

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Health

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

ETCD Operator Information:
[source, yaml]
----
apiVersion: config.openshift.io/v1
kind: ClusterOperator
metadata:
  annotations:
    exclude.release.openshift.io/internal-openshift-hosted: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
  creationTimestamp: "2025-04-14T17:52:30Z"
  generation: 1
  name: etcd
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    controller: true
    kind: ClusterVersion
    name: version
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  resourceVersion: "501907"
  uid: 1b210b7c-d7cf-4c00-b837-261015962602
spec: {}
status:
  conditions:
  - lastTransitionTime: "2025-04-14T18:01:02Z"
    message: |-
      NodeControllerDegraded: All master nodes are ready
      EtcdMembersDegraded: No unhealthy members found
    reason: AsExpected
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T18:13:56Z"
    message: |-
      NodeInstallerProgressing: 3 nodes are at revision 8
      EtcdMembersProgressing: No unstarted etcd members found
    reason: AsExpected
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-14T17:57:47Z"
    message: |-
      StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8
      EtcdMembersAvailable: 3 members are available
    reason: AsExpected
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-14T17:56:00Z"
    message: All is well
    reason: AsExpected
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:00Z"
    reason: NoData
    status: Unknown
    type: EvaluationConditionsDetected
  extension: null
  relatedObjects:
  - group: operator.openshift.io
    name: cluster
    resource: etcds
  - group: ""
    name: openshift-config
    resource: namespaces
  - group: ""
    name: openshift-config-managed
    resource: namespaces
  - group: ""
    name: openshift-etcd-operator
    resource: namespaces
  - group: ""
    name: openshift-etcd
    resource: namespaces
  versions:
  - name: raw-internal
    version: 4.16.38
  - name: etcd
    version: 4.16.38
  - name: operator
    version: 4.16.38

----

ETCD Pods Information:
[source, bash]
----
NAME                                                READY   STATUS    RESTARTS   AGE
etcd-ip-10-0-2-3.eu-central-1.compute.internal      4/4     Running   16         43h
etcd-ip-10-0-31-43.eu-central-1.compute.internal    4/4     Running   16         43h
etcd-ip-10-0-58-101.eu-central-1.compute.internal   4/4     Running   16         43h

----

ETCD Performance Information:

=== ETCD Performance Metrics ===

No 'took too long' messages found in recent logs (good).

No heartbeat issues found in recent logs (good).

No clock drift issues found in recent logs (good).

=== ETCD Performance Best Practices ===
For optimal etcd performance:
1. Use fast SSD or NVMe storage dedicated to etcd
2. Ensure 99th percentile of fsync is below 10ms
3. Network latency between etcd nodes should be below 2ms
4. CPU should not be overcommitted on etcd nodes
5. Sequential fsync IOPS should be at least 500 for medium/large clusters

**Observation**

ETCD cluster is healthy and performing well

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Performance

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== ETCD Performance Analysis ===

== Compaction Performance ==
Compaction performance is good. No slow compactions detected.


== Entry Apply Performance ==
Entry apply performance is good. No delays detected.


== Heartbeat Performance ==
Heartbeat performance is good. No issues detected.


== Clock Synchronization ==
Clock synchronization appears good. No drift issues detected.


== Node Resource Usage ==
Node resource usage appears to be within normal ranges.


== ETCD Performance Recommendations ==
For optimal etcd performance, the following are recommended:

1. Storage requirements:
   - Use dedicated SSDs (preferably NVMe) for etcd
   - For cloud environments, use high-performance storage:
     * AWS: io1/io2/io2-block-express with at least 2000 IOPS
     * Azure: Premium SSD or Ultra Disk
     * GCP: SSD Persistent Disk
   - For on-premise: avoid shared storage, use local NVMe or high-performance SSD
   - Avoid VM snapshots which can severely impact I/O performance

2. Performance metrics to aim for:
   - WAL fsync 99th percentile: < 10ms (ideally < 5ms)
   - Sequential fsync IOPS:
     * Small clusters: at least 50 IOPS
     * Medium clusters: at least 300 IOPS
     * Large clusters: at least 500 IOPS
     * Heavy workloads: at least 800 IOPS

3. Node configuration:
   - Ensure sufficient CPU resources (avoid overcommitment)
   - Configure proper time synchronization with chrony
   - Network latency between etcd nodes should be < 2ms
   - Avoid placing etcd nodes across different data centers

4. Maintenance practices:
   - Regular defragmentation during maintenance windows
   - Clean up unused projects, secrets, deployments, and other resources
   - Evaluate the impact of installing new operators


**Observation**

ETCD is performing optimally

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Proxy Settings

[cols="^"] 
|===
|
{set:cellbgcolor:#A6B9BF}
Not Applicable
|===

=== Proxy Configuration Analysis ===

Proxy Configuration:
[source, json]
----
{
    "apiVersion": "config.openshift.io/v1",
    "kind": "Proxy",
    "metadata": {
        "creationTimestamp": "2025-04-14T17:52:04Z",
        "generation": 1,
        "name": "cluster",
        "resourceVersion": "531",
        "uid": "9b46f109-6038-47c6-b68d-154f25aa7ef4"
    },
    "spec": {
        "trustedCA": {
            "name": ""
        }
    },
    "status": {}
}

----

=== Proxy Settings Summary ===

HTTP Proxy: 
HTTPS Proxy: 
No Proxy: 

**Observation**

OpenShift Proxy is not configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Taints

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Infrastructure Node Taints Analysis ===

Infrastructure Nodes:
[source, bash]
----
NAME                                           STATUS   ROLES          AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker   43h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

Taint Information:
[source, yaml]
----
ip-10-0-14-186.eu-central-1.compute.internal: 
ip-10-0-20-141.eu-central-1.compute.internal: 
ip-10-0-8-87.eu-central-1.compute.internal: 

----

=== Taint Status Summary ===

Total Infrastructure Nodes: 3
Nodes Without Taints: 3
Nodes With Insufficient Taints: 0

Nodes missing taints:
- ip-10-0-14-186.eu-central-1.compute.internal
- ip-10-0-20-141.eu-central-1.compute.internal
- ip-10-0-8-87.eu-central-1.compute.internal

=== Taint Best Practices ===

Infrastructure nodes should have appropriate taints to ensure only infrastructure workloads are scheduled on them.

Recommended taint for infrastructure nodes:
  node-role.kubernetes.io/infra:NoSchedule

This ensures regular application workloads will not be scheduled on infrastructure nodes,
while infrastructure components with matching tolerations can still be placed there.

**Observation**

3 infrastructure nodes are not tainted

**Recommendation**

Add taints to infrastructure nodes to prevent regular workloads from being scheduled on them

Use 'oc adm taint nodes <node-name> node-role.kubernetes.io/infra=:NoSchedule'

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/nodes/scheduling-pods-to-specific-nodes

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Application Probes

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Application Probes Analysis ===

Workload Statistics:
- Total User Workloads: 7
- Workloads Missing Readiness Probes: 5 (71.4%)
- Workloads Missing Liveness Probes: 3 (42.9%)
- Workloads Missing Both Probes: 3 (42.9%)

Affected Namespaces:
[source, text]
----
- cert-manager
- showroom-user1
- user1-argocd
----

Workloads Missing Probes:
[source, text]
----
- Deployment 'cert-manager' in namespace 'cert-manager' is missing readiness probe
- Deployment 'cert-manager-cainjector' in namespace 'cert-manager' is missing both readiness and liveness probes
- Deployment 'showroom' in namespace 'showroom-user1' is missing both readiness and liveness probes
- Deployment 'user1-argo-dex-server' in namespace 'user1-argocd' is missing readiness probe
- Deployment 'user1-argo-redis' in namespace 'user1-argocd' is missing both readiness and liveness probes
----

=== Probe Information ===

What are Readiness and Liveness Probes?

Readiness Probe: Determines if a container is ready to accept traffic. When a pod's readiness check fails, it is removed from service load balancers.

Liveness Probe: Determines if a container is still running as expected. When a liveness check fails, Kubernetes will restart the container.

Benefits of using probes:
- Prevents traffic from being sent to unready containers
- Automatically restarts unhealthy containers
- Improves application resilience and availability
- Facilitates smoother deployments and updates
- Provides better visibility into application health

**Observation**

Many user workloads are missing probes: 71.4% missing readiness probes, 42.9% missing liveness probes

**Recommendation**

Configure readiness and liveness probes for all user workloads

Follow the Kubernetes documentation on pod lifecycle and probes: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Resource Quotas

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Resource Quotas and Limits Analysis ===

Namespace Statistics:
- Total User Namespaces: 8
- Namespaces with Resource Quotas: 0 (0.0%)
- Namespaces with Limit Ranges: 1 (12.5%)
- Namespaces with Both: 0 (0.0%)

Namespaces Without Resource Quotas:
[source, text]
----
- cert-manager
- cert-manager-operator
- showroom-user1
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo
----

Namespaces Without Limit Ranges:
[source, text]
----
- cert-manager
- cert-manager-operator
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo
----

Namespaces Without Both Resource Quotas and Limit Ranges:
[source, text]
----
- cert-manager
- cert-manager-operator
- showroom-user1
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo
----

=== Resource Management Information ===

What are Resource Quotas and Limit Ranges?

Resource Quotas: Define the total amount of resources a namespace can use. They limit the total CPU, memory, and other resources that can be consumed by all pods in a namespace.

Limit Ranges: Define default resource limits and requests for containers in a namespace. They can also enforce minimum and maximum resource usage limits.

Benefits of using Resource Quotas and Limit Ranges:
- Prevent resource starvation by limiting the total resources a namespace can consume
- Ensure fair resource allocation across namespaces
- Protect against runaway applications that might consume all available resources
- Enforce resource constraints and prevent resource leaks
- Help with capacity planning and cost management

**Observation**

Only 0.0% of user namespaces (0 out of 8) have both resource quotas and limit ranges configured

**Recommendation**

Configure resource quotas and limit ranges for all user namespaces

Follow the Kubernetes documentation on resource quotas: https://kubernetes.io/docs/concepts/policy/resource-quotas/

Follow the Kubernetes documentation on limit ranges: https://kubernetes.io/docs/concepts/policy/limit-range/

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== EmptyDir Volumes

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== EmptyDir Volume Usage Analysis ===

Total user workloads analyzed: 9
Workloads using emptyDir volumes: 6
EmptyDir usage: 66.7%

Affected Namespaces:
[source, text]
----
- showroom-user1
- user1-argocd
----

Workloads Using EmptyDir Volumes:
[source, text]
----
- Pod 'showroom-775d8f49dc-4c2kk' in namespace 'showroom-user1' is using emptyDir volume
- Deployment 'showroom' in namespace 'showroom-user1' is using emptyDir volume
- Pod 'user1-argo-dex-server-64bcdc785b-pwb8r' in namespace 'user1-argocd' is using emptyDir volume
- Pod 'user1-argo-repo-server-55c4b88b58-njk4d' in namespace 'user1-argocd' is using emptyDir volume
- Deployment 'user1-argo-dex-server' in namespace 'user1-argocd' is using emptyDir volume
- Deployment 'user1-argo-repo-server' in namespace 'user1-argocd' is using emptyDir volume
----

=== EmptyDir Volume Information ===

What are emptyDir Volumes?

An emptyDir volume is created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.

Risks of using emptyDir volumes:
- Data loss: All data is lost when the pod is deleted or rescheduled
- No persistence across pod restarts or rescheduling
- Not suitable for stateful applications that need data persistence
- No data sharing between different pods or nodes

Recommended alternatives:
- PersistentVolumeClaims (PVCs) for persistent storage
- ConfigMaps or Secrets for configuration data
- External storage services for important data

**Observation**

66.7% of user workloads (6 out of 9) are using emptyDir volumes

**Recommendation**

Use persistent volumes instead of emptyDir for data that needs to persist

Review existing workloads using emptyDir to ensure they don't store important data

Follow the Kubernetes documentation on volumes: https://kubernetes.io/docs/concepts/storage/volumes/

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== LimitRange Configuration

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== LimitRange Configuration Analysis ===

LimitRanges Overview:
[source, bash]
----
NAMESPACE        NAME                                  CREATED AT
showroom-user1   showroom-user1-core-resource-limits   2025-04-14T18:35:21Z

----

=== Namespace Statistics ===

Total User Namespaces: 8
Namespaces with LimitRanges: 1
LimitRange Coverage: 12.5%

Namespaces Without LimitRanges:
[source, text]
----
- cert-manager
- cert-manager-operator
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo
----

=== LimitRange Information ===

LimitRange resources in Kubernetes provide constraints to limit resource consumption per container or pod in a namespace.

Benefits of using LimitRanges:
- Prevent users from creating pods that exceed specific resource limits
- Set default resource requests and limits when not specified in workloads
- Enforce minimum resource requirements for critical workloads
- Help prevent resource starvation by setting maximum constraints
- Improve overall cluster efficiency and resource utilization

**Observation**

Only 12.5% of user namespaces (1 out of 8) have LimitRange configured

**Recommendation**

Configure LimitRange resources in all namespaces to control resource usage

Set up a default project template including LimitRange

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Security Context Constraint

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Security Context Constraint Analysis ===

Restricted SCC Configuration:
[source, yaml]
----
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
groups: []
kind: SecurityContextConstraints
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    kubernetes.io/description: restricted denies access to all host features and requires
      pods to be run with a UID, and SELinux context that are allocated to the namespace.
    release.openshift.io/create-only: "true"
  creationTimestamp: "2025-04-14T17:51:48Z"
  generation: 1
  name: restricted
  resourceVersion: "345"
  uid: 7b3af7ab-1ac5-4d4a-90b1-7875dcbe0327
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- KILL
- MKNOD
- SETUID
- SETGID
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- csi
- downwardAPI
- emptyDir
- ephemeral
- persistentVolumeClaim
- projected
- secret

----

=== SCC Status ===

The default 'restricted' SCC has not been modified.

This maintains the intended security posture of the OpenShift cluster.

**Observation**

Default security context constraint (restricted) has not been modified

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Project Template

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Default Project Template Analysis ===

Project Configuration:
[source, yaml]
----
apiVersion: config.openshift.io/v1
kind: Project
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2025-04-14T17:52:49Z"
  generation: 2
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  resourceVersion: "27474"
  uid: bd1f348f-6c9e-4222-a449-f3ba1a32d57a
spec:
  projectRequestMessage: To provision Projects you must request access in https://labs.opentlc.com
    or https://rhpds.redhat.com.
  projectRequestTemplate:
    name: project-request

----

**Observation**

Default project template is configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Self Provisioner

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Self-Provisioner Role Binding Analysis ===

Self-Provisioners Role Binding:
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2025-04-14T17:58:08Z"
  name: self-provisioners
  resourceVersion: "10732"
  uid: aa2f8fe3-9217-41d0-9c88-9bf93d411015
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: self-provisioner
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:authenticated:oauth

----

Self-Provisioners Role Binding Description:
[source, bash]
----
Name:         self-provisioners
Labels:       <none>
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  self-provisioner
Subjects:
  Kind   Name                        Namespace
  ----   ----                        ---------
  Group  system:authenticated:oauth  

----

=== Security Analysis ===

The self-provisioner role binding includes 'system:authenticated:oauth' group.

This allows all authenticated users to create new projects, which may lead to uncontrolled namespace proliferation.
In enterprise environments, it's often preferred to restrict project creation to administrators.

**Observation**

Self-provisioner role binding includes system:authenticated:oauth, allowing uncontrolled namespace creation

**Recommendation**

Remove the self-provisioner role from the system:authenticated:oauth group

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/building_applications/index#disabling-project-self-provisioning_configuring-project-creation

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Kubeadmin User

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Kubeadmin User Analysis ===

Kubeadmin Secret: Not found

=== Kubeadmin Status ===

The kubeadmin user has been properly removed from the cluster.

This follows the security best practice of removing the default administrator account after setting up proper identity providers.

**Observation**

The kubeadmin user has been removed

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Identity Provider Configuration

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== Identity Provider Analysis ===

OAuth Configuration:
[source, yaml]
----
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2025-04-14T17:52:47Z"
  generation: 2
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  resourceVersion: "27389"
  uid: 69381815-0b0e-4e04-9361-e15e739c5bfe
spec:
  identityProviders:
  - htpasswd:
      fileData:
        name: htpasswd
    mappingMethod: claim
    name: htpasswd_provider
    type: HTPasswd

----

=== Identity Provider Status ===

Configured Identity Provider Types: HTPasswd

No LDAP provider found. LDAP is recommended for enterprise environments.

Current identity providers may not provide the same level of integration with existing identity management systems.

**Observation**

Identity providers are configured (HTPasswd), but no LDAP provider found

**Recommendation**

Configure a central identity provider (LDAP) for better integration with existing identity management systems

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/authentication_and_authorization/index#configuring-ldap-identity-provider

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Backup

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== ETCD Backup Analysis ===

CronJobs in the Cluster:
[source, bash]
----
NAMESPACE                              NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
openshift-image-registry               image-pruner       0 0 * * *      False     0        <none>          44h
openshift-operator-lifecycle-manager   collect-profiles   */15 * * * *   False     1        2s              44h

----

ETCD Cluster Operator Status:
[source, bash]
----
NAME   VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
etcd   4.16.38   True        False         False      44h     

----

=== Backup Status ===

No CronJobs found that might be backing up etcd

Regular etcd backups are critical for disaster recovery. Without backups, cluster recovery options are limited.

**Observation**

No CronJobs found that might be backing up etcd

**Recommendation**

Set up regular etcd backups to protect against data loss

Follow the documentation at https://docs.openshift.com/container-platform/latest/backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.html

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Encryption

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

=== ETCD Encryption Analysis ===

API Server Configuration:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: APIServer
  metadata:
    annotations:
      include.release.openshift.io/ibm-cloud-managed: "true"
      include.release.openshift.io/self-managed-high-availability: "true"
      oauth-apiserver.openshift.io/secure-token-storage: "true"
      release.openshift.io/create-only: "true"
    creationTimestamp: "2025-04-14T17:52:31Z"
    generation: 1
    name: cluster
    ownerReferences:
    - apiVersion: config.openshift.io/v1
      kind: ClusterVersion
      name: version
      uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
    resourceVersion: "767"
    uid: fd6ddf91-b4b6-434a-aa27-135c36d4914b
  spec:
    audit:
      profile: Default
kind: List
metadata:
  resourceVersion: ""

----

=== Encryption Status ===

ETCD encryption is NOT enabled

Without encryption, sensitive data in etcd is stored in plaintext, which could be a security risk.

**Observation**

ETCD encryption is not enabled

**Recommendation**

Enable etcd encryption to protect sensitive data

Follow the documentation at https://docs.openshift.com/container-platform/latest/security/encrypting-etcd.html

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Elevated Privileges

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== Elevated Privileges Analysis ===

No user workloads with elevated privileges were found.

This is good and follows the principle of least privilege, reducing the security risk surface.

**Observation**

No user workloads using privileged containers were found

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Monitoring Stack Configuration

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

== Monitoring Stack Configuration Analysis ==

Cluster Monitoring Config exists:

[source, yaml]
----
apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"config.yaml":"enableUserWorkload: true\nprometheusOperator:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nnodeExporter:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 150Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\nprometheusK8s:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 3Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  retention: 7d\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 40Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nalertmanagerMain:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 10Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nmonitoringPlugin:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nopenshiftStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\ntelemeterClient:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nk8sPrometheusAdapter:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nthanosQuerier:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nprometheusOperatorAdmissionWebhook:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 100Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"cluster-monitoring-config","namespace":"openshift-monitoring"}}
  creationTimestamp: "2025-04-15T10:31:14Z"
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  resourceVersion: "328571"
  uid: 11f1e0ef-c0da-4948-99d0-4425e5996ebe

----

== Monitoring Stack Components ==

Using OpenShift 4.16 monitoring component requirements

Components explicitly configured in cluster-monitoring-config:
- prometheusOperator
- prometheusK8s
- alertmanagerMain
- thanosQuerier
- kubeStateMetrics
- monitoringPlugin
- openshiftStateMetrics
- telemeterClient
- nodeExporter
- prometheusOperatorAdmissionWebhook

WARNING: The following core components are NOT explicitly configured in cluster-monitoring-config:
- metricsServer

These components are running with default settings, which may not be optimal for your environment.
Consider configuring these components explicitly for better control over resources and behavior.

User Workload Monitoring enabled: true


Verifying component status in the cluster:

- prometheusOperator: ✅ Running

- prometheusK8s: ✅ Running

- alertmanagerMain: ✅ Running

- thanosQuerier: ✅ Running

- kubeStateMetrics: ✅ Running

- monitoringPlugin: ✅ Running

- openshiftStateMetrics: ✅ Running

- telemeterClient: ✅ Running

- metricsServer: ✅ Running

- nodeExporter: ✅ Running

- prometheusOperatorAdmissionWebhook: ✅ Running



== Persistent Storage Configuration ==

[source, yaml]
----
PVCs in openshift-monitoring namespace:
NAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
alertmanager-main-db-alertmanager-main-0   Bound    pvc-5891c087-769f-426f-a925-be04857ef3eb   10Gi       RWO            gp3-csi        <unset>                 27h
alertmanager-main-db-alertmanager-main-1   Bound    pvc-0e7468f7-9013-4781-a378-e7504c88de8b   10Gi       RWO            gp3-csi        <unset>                 27h
prometheus-k8s-db-prometheus-k8s-0         Bound    pvc-a34c3fdb-8967-441e-ac6e-3a50559c074b   10Gi       RWO            gp3-csi        <unset>                 27h
prometheus-k8s-db-prometheus-k8s-1         Bound    pvc-50ef79db-6124-485a-b50d-591930fb8a4f   10Gi       RWO            gp3-csi        <unset>                 27h

Persistent storage is configured in the monitoring config via volumeClaimTemplate
Prometheus PVC found
Alertmanager PVC found
----

== Resource Requests and Limits ==

[source, yaml]
----
Resource requests and/or limits are configured in the monitoring config

Resource configuration for prometheus:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
----

== Node Placement Configuration ==

[source, yaml]
----
Component prometheusOperator has nodeSelector: true, tolerations: true
Component prometheusK8s has nodeSelector: true, tolerations: true
Component alertmanagerMain has nodeSelector: true, tolerations: true
Component thanosQuerier has nodeSelector: true, tolerations: true
Component kubeStateMetrics has nodeSelector: true, tolerations: true
Component monitoringPlugin has nodeSelector: true, tolerations: true
Component openshiftStateMetrics has nodeSelector: true, tolerations: true
Component telemeterClient has nodeSelector: true, tolerations: true
Component metricsServer is not configured in the monitoring config

WARNING: Missing node placement configuration for components: metricsServer

Infrastructure nodes found in the cluster

Monitoring pods are running on infrastructure nodes
----

== Data Retention Configuration ==

[source, yaml]
----
Prometheus data retention is configured

Retention time: retention: 7d
No retention size limit configured
----

== Remote Write Configuration ==

[source, yaml]
----
No remote write configuration found
Consider configuring remote write for long-term metrics storage
----

== Alert Routing Configuration ==

[source, yaml]
----
Alertmanager configuration:
"inhibit_rules":
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = critical"
  "target_matchers":
  - "severity =~ warning|info"
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = warning"
  "target_matchers":
  - "severity = info"
"receivers":
- "name": "Default"
- "name": "Watchdog"
- "name": "Critical"
"route":
  "group_by":
  - "namespace"
  "group_interval": "5m"
  "group_wait": "30s"
  "receiver": "Default"
  "repeat_interval": "12h"
  "routes":
  - "matchers":
    - "alertname = Watchdog"
    "receiver": "Watchdog"
  - "matchers":
    - "severity = critical"
    "receiver": "Critical"

No custom alert receivers found beyond default configuration
Configure alert receivers to ensure notifications are sent to the right teams

No specific alert receiver configurations found
----

== Best Practices Recommendations ==

1. For production clusters, configure persistent storage for monitoring components
2. Set appropriate CPU and memory limits for monitoring components
3. Configure node placement to isolate monitoring components on dedicated nodes
4. Set appropriate retention time and size for Prometheus data
5. Configure remote write for long-term metrics storage
6. Set up alert routing to ensure notifications reach the right people
7. Enable user workload monitoring for application metrics

== Sample Configuration ==

Below is a sample configuration for the cluster-monitoring-config ConfigMap that includes proper resource limits, storage configuration, and node placement:

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
----

Note: The above is a simplified example. You should adjust resource limits, storage size, and other parameters based on your cluster size and workload.

**Observation**

Monitoring stack configuration needs improvement: monitoring stack is missing configuration for components: metricsServer, node placement configuration incomplete for components: metricsServer, remote write storage not configured for long-term metrics retention

**Recommendation**

Configure all recommended monitoring stack components for comprehensive monitoring

Configure nodeSelector and tolerations for all monitoring components to place them on infrastructure nodes

Configure remote write storage for long-term metrics retention and historical analysis

Configure alert routing to ensure alerts are properly delivered to the right teams

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/monitoring/index

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Logging Forwarders OPS

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

Logging Type: Loki-based logging

Log Forwarder Configuration:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: observability.openshift.io/v1
  kind: ClusterLogForwarder
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"observability.openshift.io/v1","kind":"ClusterLogForwarder","metadata":{"annotations":{},"name":"instance","namespace":"openshift-logging"},"spec":{"outputs":[{"lokiStack":{"authentication":{"token":{"from":"serviceAccount"}},"target":{"name":"logging-loki","namespace":"openshift-logging"}},"name":"lokistack-out","tls":{"ca":{"configMapName":"openshift-service-ca.crt","key":"service-ca.crt"}},"type":"lokiStack"}],"pipelines":[{"inputRefs":["application","infrastructure"],"name":"infra-app-logs","outputRefs":["lokistack-out"]}],"serviceAccount":{"name":"logging-collector"}}}
    creationTimestamp: "2025-04-14T18:53:57Z"
    generation: 1
    name: instance
    namespace: openshift-logging
    resourceVersion: "804738"
    uid: 8d070534-f15d-4899-b2ae-48290af92d33
  spec:
    managementState: Managed
    outputs:
    - lokiStack:
        authentication:
          token:
            from: serviceAccount
        target:
          name: logging-loki
          namespace: openshift-logging
      name: lokistack-out
      tls:
        ca:
          configMapName: openshift-service-ca.crt
          key: service-ca.crt
      type: lokiStack
    pipelines:
    - inputRefs:
      - application
      - infrastructure
      name: infra-app-logs
      outputRefs:
      - lokistack-out
    serviceAccount:
      name: logging-collector
  status:
    conditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: 'permitted to collect log types: [application infrastructure]'
      reason: ClusterRolesExist
      status: "True"
      type: observability.openshift.io/Authorized
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: ""
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/Valid
    - lastTransitionTime: "2025-04-16T03:51:07Z"
      message: ""
      reason: ReconciliationComplete
      status: "True"
      type: Ready
    inputConditions:
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: input "application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-application
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: input "infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-infrastructure
    outputConditions:
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: output "lokistack-out-application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-application
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: output "lokistack-out-infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-infrastructure
    pipelineConditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: pipeline "infra-app-logs" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: pipeline "infra-app-logs-1" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs-1
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

No external log forwarding is configured for operations logs

**Recommendation**

Configure external forwarding for infrastructure and audit logs

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-external

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Log Forwarding

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

Logging Type: Loki-based logging

Log Forwarder Configuration:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: observability.openshift.io/v1
  kind: ClusterLogForwarder
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"observability.openshift.io/v1","kind":"ClusterLogForwarder","metadata":{"annotations":{},"name":"instance","namespace":"openshift-logging"},"spec":{"outputs":[{"lokiStack":{"authentication":{"token":{"from":"serviceAccount"}},"target":{"name":"logging-loki","namespace":"openshift-logging"}},"name":"lokistack-out","tls":{"ca":{"configMapName":"openshift-service-ca.crt","key":"service-ca.crt"}},"type":"lokiStack"}],"pipelines":[{"inputRefs":["application","infrastructure"],"name":"infra-app-logs","outputRefs":["lokistack-out"]}],"serviceAccount":{"name":"logging-collector"}}}
    creationTimestamp: "2025-04-14T18:53:57Z"
    generation: 1
    name: instance
    namespace: openshift-logging
    resourceVersion: "804738"
    uid: 8d070534-f15d-4899-b2ae-48290af92d33
  spec:
    managementState: Managed
    outputs:
    - lokiStack:
        authentication:
          token:
            from: serviceAccount
        target:
          name: logging-loki
          namespace: openshift-logging
      name: lokistack-out
      tls:
        ca:
          configMapName: openshift-service-ca.crt
          key: service-ca.crt
      type: lokiStack
    pipelines:
    - inputRefs:
      - application
      - infrastructure
      name: infra-app-logs
      outputRefs:
      - lokistack-out
    serviceAccount:
      name: logging-collector
  status:
    conditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: 'permitted to collect log types: [application infrastructure]'
      reason: ClusterRolesExist
      status: "True"
      type: observability.openshift.io/Authorized
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: ""
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/Valid
    - lastTransitionTime: "2025-04-16T03:51:07Z"
      message: ""
      reason: ReconciliationComplete
      status: "True"
      type: Ready
    inputConditions:
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: input "application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-application
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: input "infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-infrastructure
    outputConditions:
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: output "lokistack-out-application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-application
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: output "lokistack-out-infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-infrastructure
    pipelineConditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: pipeline "infra-app-logs" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: pipeline "infra-app-logs-1" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs-1
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

No external log forwarding configured

**Recommendation**

Configure external log forwarding for long-term storage and better log management

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-external

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Logging Installation

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

Cluster Log Forwarder Configuration:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: observability.openshift.io/v1
  kind: ClusterLogForwarder
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"observability.openshift.io/v1","kind":"ClusterLogForwarder","metadata":{"annotations":{},"name":"instance","namespace":"openshift-logging"},"spec":{"outputs":[{"lokiStack":{"authentication":{"token":{"from":"serviceAccount"}},"target":{"name":"logging-loki","namespace":"openshift-logging"}},"name":"lokistack-out","tls":{"ca":{"configMapName":"openshift-service-ca.crt","key":"service-ca.crt"}},"type":"lokiStack"}],"pipelines":[{"inputRefs":["application","infrastructure"],"name":"infra-app-logs","outputRefs":["lokistack-out"]}],"serviceAccount":{"name":"logging-collector"}}}
    creationTimestamp: "2025-04-14T18:53:57Z"
    generation: 1
    name: instance
    namespace: openshift-logging
    resourceVersion: "804738"
    uid: 8d070534-f15d-4899-b2ae-48290af92d33
  spec:
    managementState: Managed
    outputs:
    - lokiStack:
        authentication:
          token:
            from: serviceAccount
        target:
          name: logging-loki
          namespace: openshift-logging
      name: lokistack-out
      tls:
        ca:
          configMapName: openshift-service-ca.crt
          key: service-ca.crt
      type: lokiStack
    pipelines:
    - inputRefs:
      - application
      - infrastructure
      name: infra-app-logs
      outputRefs:
      - lokistack-out
    serviceAccount:
      name: logging-collector
  status:
    conditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: 'permitted to collect log types: [application infrastructure]'
      reason: ClusterRolesExist
      status: "True"
      type: observability.openshift.io/Authorized
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: ""
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/Valid
    - lastTransitionTime: "2025-04-16T03:51:07Z"
      message: ""
      reason: ReconciliationComplete
      status: "True"
      type: Ready
    inputConditions:
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: input "application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-application
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: input "infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-infrastructure
    outputConditions:
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: output "lokistack-out-application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-application
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: output "lokistack-out-infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-infrastructure
    pipelineConditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: pipeline "infra-app-logs" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs
    - lastTransitionTime: "2025-04-16T13:59:44Z"
      message: pipeline "infra-app-logs-1" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs-1
kind: List
metadata:
  resourceVersion: ""

----

Loki Stack Configuration:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: loki.grafana.com/v1
  kind: LokiStack
  metadata:
    annotations:
      loki.grafana.com/rulesDiscoveredAt: "2025-04-16T12:55:29Z"
    creationTimestamp: "2025-04-14T18:51:50Z"
    generation: 1
    name: lokistack
    namespace: openshift-logging
    resourceVersion: "766734"
    uid: bdbeca6f-5360-47be-abcc-082bcaf90ec9
  spec:
    hashRing:
      type: memberlist
    limits:
      global:
        queries:
          queryTimeout: 3m
    managementState: Managed
    size: 1x.extra-small
    storage:
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
      secret:
        name: logging-loki-s3
        type: s3
    storageClassName: gp3-csi
    tenants:
      mode: openshift-logging
  status:
    components:
      compactor:
        Ready:
        - lokistack-compactor-0
      distributor:
        Ready:
        - lokistack-distributor-756f98f45b-2j5zx
        - lokistack-distributor-756f98f45b-ssvd8
      gateway:
        Ready:
        - lokistack-gateway-5b6f4ccdfb-rg96z
        - lokistack-gateway-5b6f4ccdfb-v54nd
      indexGateway:
        Ready:
        - lokistack-index-gateway-1
        - lokistack-index-gateway-0
      ingester:
        Ready:
        - lokistack-ingester-1
        - lokistack-ingester-0
      querier:
        Ready:
        - lokistack-querier-64b9849496-5j7mt
        - lokistack-querier-64b9849496-tfrp8
      queryFrontend:
        Ready:
        - lokistack-query-frontend-6bffbfbf-5sfk5
        - lokistack-query-frontend-6bffbfbf-8xk5n
      ruler:
        Failed: []
        Pending: []
        Ready: []
        Running: []
    conditions:
    - lastTransitionTime: "2025-04-16T12:55:40Z"
      message: The schema configuration does not contain the most recent schema version
        and needs an update
      reason: StorageNeedsSchemaUpdate
      status: "True"
      type: Warning
    - lastTransitionTime: "2025-04-16T12:55:40Z"
      message: All components ready
      reason: ReadyComponents
      status: "True"
      type: Ready
    - lastTransitionTime: "2025-04-16T12:55:40Z"
      message: All components are running, but some readiness checks are failing
      reason: PendingComponents
      status: "False"
      type: Pending
    storage:
      credentialMode: static
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

OpenShift Logging with Loki is installed and configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Logging Health

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

Logging Type: Loki-based logging

Logging Pods Status: No information available

Collector Pods Status:
[source, bash]
----
NAME             READY   STATUS    RESTARTS   AGE
instance-8w2hx   1/1     Running   4          43h
instance-ckxqg   1/1     Running   4          43h
instance-glms2   1/1     Running   4          43h
instance-hf9dw   1/1     Running   4          43h
instance-wr9s5   1/1     Running   4          43h
instance-zzpkz   1/1     Running   4          43h

----

**Observation**

Loki Logging is healthy

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Logging Component Placement

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

Logging Type: Loki-based logging

Infrastructure Nodes:
[source, bash]
----
node/ip-10-0-14-186.eu-central-1.compute.internal
node/ip-10-0-20-141.eu-central-1.compute.internal
node/ip-10-0-8-87.eu-central-1.compute.internal

----

Logging Component Pods: No information available

Placement Analysis:
All logging component pods are correctly placed on infrastructure nodes.

**Observation**

All Loki pods are scheduled on infrastructure nodes

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Logging Storage

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

Logging Type: Loki-based logging

LokiStack Configuration:
[source, yaml]
----
apiVersion: v1
items:
- apiVersion: loki.grafana.com/v1
  kind: LokiStack
  metadata:
    annotations:
      loki.grafana.com/rulesDiscoveredAt: "2025-04-16T12:55:29Z"
    creationTimestamp: "2025-04-14T18:51:50Z"
    generation: 1
    name: lokistack
    namespace: openshift-logging
    resourceVersion: "766734"
    uid: bdbeca6f-5360-47be-abcc-082bcaf90ec9
  spec:
    hashRing:
      type: memberlist
    limits:
      global:
        queries:
          queryTimeout: 3m
    managementState: Managed
    size: 1x.extra-small
    storage:
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
      secret:
        name: logging-loki-s3
        type: s3
    storageClassName: gp3-csi
    tenants:
      mode: openshift-logging
  status:
    components:
      compactor:
        Ready:
        - lokistack-compactor-0
      distributor:
        Ready:
        - lokistack-distributor-756f98f45b-2j5zx
        - lokistack-distributor-756f98f45b-ssvd8
      gateway:
        Ready:
        - lokistack-gateway-5b6f4ccdfb-rg96z
        - lokistack-gateway-5b6f4ccdfb-v54nd
      indexGateway:
        Ready:
        - lokistack-index-gateway-1
        - lokistack-index-gateway-0
      ingester:
        Ready:
        - lokistack-ingester-1
        - lokistack-ingester-0
      querier:
        Ready:
        - lokistack-querier-64b9849496-5j7mt
        - lokistack-querier-64b9849496-tfrp8
      queryFrontend:
        Ready:
        - lokistack-query-frontend-6bffbfbf-5sfk5
        - lokistack-query-frontend-6bffbfbf-8xk5n
      ruler:
        Failed: []
        Pending: []
        Ready: []
        Running: []
    conditions:
    - lastTransitionTime: "2025-04-16T12:55:40Z"
      message: The schema configuration does not contain the most recent schema version
        and needs an update
      reason: StorageNeedsSchemaUpdate
      status: "True"
      type: Warning
    - lastTransitionTime: "2025-04-16T12:55:40Z"
      message: All components ready
      reason: ReadyComponents
      status: "True"
      type: Ready
    - lastTransitionTime: "2025-04-16T12:55:40Z"
      message: All components are running, but some readiness checks are failing
      reason: PendingComponents
      status: "False"
      type: Pending
    storage:
      credentialMode: static
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
kind: List
metadata:
  resourceVersion: ""

----

Storage Type: Object Storage (S3, GCS, or Azure)

**Observation**

Loki storage schema needs to be updated

**Recommendation**

Update the Loki storage schema to the latest version

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/logging/index

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Service Monitors

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

=== User Workload Monitoring Status ===

User Workload Monitoring is ENABLED in the cluster

User Workload Monitoring Status:
[source, yaml]
----
Namespace Status:
NAME                                 STATUS   AGE
openshift-user-workload-monitoring   Active   44h


Config Status:
apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"config.yaml":"enableUserWorkload: true\nprometheusOperator:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nnodeExporter:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 150Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\nprometheusK8s:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 3Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  retention: 7d\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 40Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nalertmanagerMain:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 10Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nmonitoringPlugin:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nopenshiftStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\ntelemeterClient:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nk8sPrometheusAdapter:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nthanosQuerier:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nprometheusOperatorAdmissionWebhook:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 100Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"cluster-monitoring-config","namespace":"openshift-monitoring"}}
  creationTimestamp: "2025-04-15T10:31:14Z"
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  resourceVersion: "328571"
  uid: 11f1e0ef-c0da-4948-99d0-4425e5996ebe

----

=== ServiceMonitor Check Results ===

User ServiceMonitors found in the cluster:

- Namespace: user1-argocd, Name: user1-argo
- Namespace: user1-argocd, Name: user1-argo-repo-server
- Namespace: user1-argocd, Name: user1-argo-server

Sample ServiceMonitor configuration:
[source, yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  creationTimestamp: "2025-04-14T18:33:17Z"
  generation: 1
  labels:
    release: prometheus-operator
  name: user1-argo
  namespace: user1-argocd
  ownerReferences:
  - apiVersion: argoproj.io/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: ArgoCD
    name: user1-argo
    uid: 77533e0f-d0fd-47ae-8d10-2648af5bc9ed
  resourceVersion: "37652"
  uid: 9d6475a6-91ce-44c2-820a-d54fac1df5ef
spec:
  endpoints:
  - bearerTokenSecret:
      key: ""
    port: metrics
  namespaceSelector: {}
  selector:
    matchLabels:
      app.kubernetes.io/name: user1-argo-metrics

----

**Observation**

Found 3 ServiceMonitors for application metrics monitoring

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Alerts Forwarding

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, text]
----
Alertmanager configuration:
"inhibit_rules":
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = critical"
  "target_matchers":
  - "severity =~ warning|info"
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = warning"
  "target_matchers":
  - "severity = info"
"receivers":
- "name": "Default"
- "name": "Watchdog"
- "name": "Critical"
"route":
  "group_by":
  - "namespace"
  "group_interval": "5m"
  "group_wait": "30s"
  "receiver": "Default"
  "repeat_interval": "12h"
  "routes":
  - "matchers":
    - "alertname = Watchdog"
    "receiver": "Watchdog"
  - "matchers":
    - "severity = critical"
    "receiver": "Critical"
----
**Observation**

No external alert forwarding configured

**Recommendation**

Configure external receivers for alerts to ensure notifications are sent to the right teams

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/monitoring/index#sending-notifications-to-external-systems_managing-alerts

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Monitoring Storage

[cols="^"] 
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

Monitoring Namespace: openshift-monitoring

ConfigMap cluster-monitoring-config exists with config.yaml:
[source, yaml]
----
enableUserWorkload: true
prometheusOperator:
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 500Mi
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
nodeExporter:
  resources:
    limits:
      cpu: 50m
      memory: 150Mi
    requests:
      cpu: 20m
      memory: 50Mi
prometheusK8s:
  resources:
    limits:
      cpu: 500m
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 500Mi
  retention: 7d
  volumeClaimTemplate:
    spec:
      storageClassName: gp3-csi
      resources:
        requests:
          storage: 40Gi
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
alertmanagerMain:
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 500Mi
  volumeClaimTemplate:
    spec:
      storageClassName: gp3-csi
      resources:
        requests:
          storage: 10Gi
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
kubeStateMetrics:
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 500Mi
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
monitoringPlugin:
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
openshiftStateMetrics:
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 500Mi
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
telemeterClient:
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 500Mi
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
k8sPrometheusAdapter:
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
thanosQuerier:
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 500Mi
  nodeSelector:
    node-role.kubernetes.io/infra: ""
  tolerations:
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoSchedule
  - key: node-role.kubernetes.io/infra
    value: reserved
    effect: NoExecute
prometheusOperatorAdmissionWebhook:
  resources:
    limits:
      cpu: 50m
      memory: 100Mi
    requests:
      cpu: 20m
      memory: 50Mi

----

Persistent Volume Claims in openshift-monitoring:
[source, bash]
----
NAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
alertmanager-main-db-alertmanager-main-0   Bound    pvc-5891c087-769f-426f-a925-be04857ef3eb   10Gi       RWO            gp3-csi        <unset>                 27h
alertmanager-main-db-alertmanager-main-1   Bound    pvc-0e7468f7-9013-4781-a378-e7504c88de8b   10Gi       RWO            gp3-csi        <unset>                 27h
prometheus-k8s-db-prometheus-k8s-0         Bound    pvc-a34c3fdb-8967-441e-ac6e-3a50559c074b   10Gi       RWO            gp3-csi        <unset>                 27h
prometheus-k8s-db-prometheus-k8s-1         Bound    pvc-50ef79db-6124-485a-b50d-591930fb8a4f   10Gi       RWO            gp3-csi        <unset>                 27h

----

Storage Analysis: Persistent storage is configured for monitoring components

**Observation**

OpenShift monitoring components have persistent storage configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== User Workload Monitoring

[cols="^"] 
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

== User Workload Monitoring Configuration ==

Cluster Monitoring ConfigMap:
[source, yaml]
----
apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"config.yaml":"enableUserWorkload: true\nprometheusOperator:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nnodeExporter:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 150Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\nprometheusK8s:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 3Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  retention: 7d\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 40Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nalertmanagerMain:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 10Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nmonitoringPlugin:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nopenshiftStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\ntelemeterClient:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nk8sPrometheusAdapter:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nthanosQuerier:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nprometheusOperatorAdmissionWebhook:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 100Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"cluster-monitoring-config","namespace":"openshift-monitoring"}}
  creationTimestamp: "2025-04-15T10:31:14Z"
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  resourceVersion: "328571"
  uid: 11f1e0ef-c0da-4948-99d0-4425e5996ebe

----

User Workload Monitoring enabled in config: true

User Workload Monitoring namespace exists: true

== User Workload Monitoring Components ==

User Workload Monitoring ConfigMap:
[source, yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: "2025-04-15T10:21:12Z"
  labels:
    app.kubernetes.io/managed-by: cluster-monitoring-operator
    app.kubernetes.io/part-of: openshift-monitoring
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
  resourceVersion: "260740"
  uid: b61d5ceb-3362-4b84-82a1-95b9bd578f87

----

WARNING: No components are explicitly configured in the user-workload-monitoring-config ConfigMap.
All components are using default settings which may not be optimal for production.

WARNING: The following required components are NOT explicitly configured:

- prometheusOperator
- prometheus
- thanosRuler
- alertmanager

These components are running with default settings, which may not be optimal for your environment.
Consider configuring these components explicitly for better control over resources and behavior.

=== Component Status in the Cluster ===

- prometheusOperator: ✅ Running
- prometheus: ✅ Running
- thanosRuler: ✅ Running
- alertmanager: ❌ Not found or not ready

User Workload Monitoring Pods:
[source, bash]
----
NAME                                   READY   STATUS    RESTARTS   AGE   IP            NODE                                           NOMINATED NODE   READINESS GATES
prometheus-operator-7cdb67859f-rxmnj   2/2     Running   6          27h   10.129.0.35   ip-10-0-2-3.eu-central-1.compute.internal      <none>           <none>
prometheus-user-workload-0             6/6     Running   18         27h   10.129.2.57   ip-10-0-20-141.eu-central-1.compute.internal   <none>           <none>
prometheus-user-workload-1             6/6     Running   18         27h   10.131.0.67   ip-10-0-14-186.eu-central-1.compute.internal   <none>           <none>
thanos-ruler-user-workload-0           4/4     Running   12         27h   10.131.0.64   ip-10-0-14-186.eu-central-1.compute.internal   <none>           <none>
thanos-ruler-user-workload-1           4/4     Running   12         27h   10.129.2.51   ip-10-0-20-141.eu-central-1.compute.internal   <none>           <none>

----

**Observation**

User Workload Monitoring is enabled but missing configuration for components: prometheusOperator, prometheus, thanosRuler, alertmanager

**Recommendation**

Configure all required components for User Workload Monitoring

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/monitoring/index#configuring-the-monitoring-stack

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

// Reset bgcolor for future tables
[grid=none,frame=none]
|===
|{set:cellbgcolor!}
|===

