= OpenShift Health Check Report

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Key

[cols="1,3", options=header]
|===
|Value
|Description

|
{set:cellbgcolor:#FF0000}
Changes Required
|
{set:cellbgcolor!}
Indicates Changes Required for system stability, subscription compliance, or other reason.

|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|
{set:cellbgcolor!}
Indicates Changes Recommended to align with recommended practices, but not urgently required

|
{set:cellbgcolor:#A6B9BF}
N/A
|
{set:cellbgcolor!}
No advise given on line item.  For line items which are data-only to provide context.

|
{set:cellbgcolor:#80E5FF}
Advisory
|
{set:cellbgcolor!}
No change required or recommended, but additional information provided.

|
{set:cellbgcolor:#00FF00}
No Change
|
{set:cellbgcolor!}
No change required. In alignment with recommended practices.

|
{set:cellbgcolor:#FFFFFF}
To Be Evaluated
|
{set:cellbgcolor!}
Not yet evaluated. Will appear only in draft copies.
|===

= Summary


[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-type.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Type>>

| Default OpenShift ingress controller is in use 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-placement.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Placement>>

| Ingress controller node placement is not configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-replica.item

// Category
|
{set:cellbgcolor!}
Network

// Item Evaluated
a|
<<Ingress Controller Replicas>>

| Ingress controller has insufficient replicas: 2 (recommended: >= 3) 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-ingress-certificate.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Default Ingress Certificate>>

| Custom certificate is properly configured for the default ingress controller 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cni-network-plugin.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<CNI Network Plugin>>

| Cluster is using the recommended CNI network plugin: OVNKubernetes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/network-policy.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Network Policy>>

| Found 6 network policies in the cluster 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller>>

| Ingress controller has 2 configuration issues 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-classes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Classes>>

| Default storage class 'gp3-csi' is configured and RWX-capable storage is available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/persistent-volumes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Persistent Volumes>>

| All 19 persistent volumes are healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-performance.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Performance>>

| No storage classes with explicit performance characteristics found 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-provider.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Provider>>

| Infrastructure provider type: AWS 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/installation-type.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Installation Type>>

| Installation type: IPI 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/upi-machinesets.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<UPI with MachineSets>>

| This is an IPI installation, UPI MachineSets check is not applicable 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-status.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Status>>

| All 6 nodes are ready 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-usage.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Usage>>

| All nodes are within resource usage thresholds 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-version.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Version>>

| Cluster version 4.16.38 is up to date 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-operators.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Operators>>

| All cluster operators are available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/control-node-schedulable.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Control Plane Node Schedulability>>

| All control plane nodes are properly configured to prevent regular workloads 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Nodes>>

| Found 3 infrastructure nodes but not all are properly tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/workload-off-infra-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Workloads on Infrastructure Nodes>>

| 10 user workloads are running on infrastructure nodes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-node-schedule.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Default Node Schedule>>

| No custom namespace node selectors are configured 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-machine-config-pool.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Machine Config Pool>>

| No dedicated infrastructure machine config pool found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubelet-garbage-collection.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Kubelet Garbage Collection>>

| No custom kubelet garbage collection configuration found (using defaults) 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-health.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Health>>

| ETCD cluster is healthy and performing well 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-performance.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Performance>>

| ETCD is performing optimally 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/proxy-settings.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<OpenShift Proxy Settings>>

| OpenShift Proxy is not configured 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-taints.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Taints>>

| 3 infrastructure nodes are not tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/application-probes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Application Probes>>

| Many user workloads are missing probes: 71.4% missing readiness probes, 42.9% missing liveness probes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/resource-quotas.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Resource Quotas>>

| Only 0.0% of user namespaces (0 out of 8) have both resource quotas and limit ranges configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/emptydir-volumes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<EmptyDir Volumes>>

| 66.7% of user workloads (6 out of 9) are using emptyDir volumes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/limit-range.item

// Category
|
{set:cellbgcolor!}
App Dev

// Item Evaluated
a|
<<LimitRange Configuration>>

| Only 12.5% of user namespaces (1 out of 8) have LimitRange configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-default-scc.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Security Context Constraint>>

| Default security context constraint (restricted) has not been modified 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-project-template.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Project Template>>

| Default project template is configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/self-provisioner.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Self Provisioner>>

| Self-provisioner role binding includes system:authenticated:oauth, allowing uncontrolled namespace creation 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubeadmin-user.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Kubeadmin User>>

| The kubeadmin user has been removed 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/identity-provider.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Identity Provider Configuration>>

| Identity providers are configured (HTPasswd), but no LDAP provider found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-backup.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Backup>>

| No CronJobs found that might be backing up etcd 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-encryption.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Encryption>>

| ETCD encryption is not enabled 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/elevated-privileges.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Elevated Privileges>>

| No user workloads using privileged containers were found 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-stack-config.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Stack Configuration>>

| Monitoring stack configuration needs improvement: monitoring stack is missing configuration for components: metricsServer, node placement configuration incomplete for components: metricsServer, remote write storage not configured for long-term metrics retention 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarders-ops.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Forwarders OPS>>

| No external log forwarding is configured for operations logs 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarder.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Log Forwarding>>

| Loki logging is configured but no external forwarding is set up 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-install.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Installation>>

| OpenShift Logging with Loki is installed and configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-health.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Health>>

| Loki Logging is healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-placement.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Component Placement>>

| All Loki pods are scheduled on infrastructure nodes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Storage>>

| Loki storage schema needs to be updated 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/service-monitors.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Service Monitors>>

| Found 3 ServiceMonitors for application metrics monitoring 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/alerts-forwarding.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Alerts Forwarding>>

| No external alert forwarding configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Storage>>

| OpenShift monitoring components have persistent storage configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/user-workload-monitoring.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<User Workload Monitoring>>

| User Workload Monitoring is enabled but missing configuration for components: prometheusOperator, prometheus, thanosRuler, alertmanager 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END

|===

<<<

{set:cellbgcolor!}

# Network

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-type.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Type>>

| Default OpenShift ingress controller is in use 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-placement.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller Placement>>

| Ingress controller node placement is not configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller-replica.item

// Category
|
{set:cellbgcolor!}
Network

// Item Evaluated
a|
<<Ingress Controller Replicas>>

| Ingress controller has insufficient replicas: 2 (recommended: >= 3) 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-ingress-certificate.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Default Ingress Certificate>>

| Custom certificate is properly configured for the default ingress controller 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cni-network-plugin.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<CNI Network Plugin>>

| Cluster is using the recommended CNI network plugin: OVNKubernetes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/network-policy.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Network Policy>>

| Found 6 network policies in the cluster 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/ingress-controller.item

// Category
|
{set:cellbgcolor!}
Networking

// Item Evaluated
a|
<<Ingress Controller>>

| Ingress controller has 2 configuration issues 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Storage

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-classes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Classes>>

| Default storage class 'gp3-csi' is configured and RWX-capable storage is available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/persistent-volumes.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Persistent Volumes>>

| All 19 persistent volumes are healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/storage-performance.item

// Category
|
{set:cellbgcolor!}
Storage

// Item Evaluated
a|
<<Storage Performance>>

| No storage classes with explicit performance characteristics found 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Cluster Config

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-provider.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Provider>>

| Infrastructure provider type: AWS 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/installation-type.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Installation Type>>

| Installation type: IPI 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/upi-machinesets.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<UPI with MachineSets>>

| This is an IPI installation, UPI MachineSets check is not applicable 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-status.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Status>>

| All 6 nodes are ready 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/node-usage.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Node Usage>>

| All nodes are within resource usage thresholds 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-version.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Version>>

| Cluster version 4.16.38 is up to date 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-operators.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Cluster Operators>>

| All cluster operators are available 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/control-node-schedulable.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Control Plane Node Schedulability>>

| All control plane nodes are properly configured to prevent regular workloads 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infrastructure-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Nodes>>

| Found 3 infrastructure nodes but not all are properly tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/workload-off-infra-nodes.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Workloads on Infrastructure Nodes>>

| 10 user workloads are running on infrastructure nodes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-node-schedule.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Default Node Schedule>>

| No custom namespace node selectors are configured 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-machine-config-pool.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Machine Config Pool>>

| No dedicated infrastructure machine config pool found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubelet-garbage-collection.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Kubelet Garbage Collection>>

| No custom kubelet garbage collection configuration found (using defaults) 

|{set:cellbgcolor:#80E5FF}
Advisory


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-health.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Health>>

| ETCD cluster is healthy and performing well 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-performance.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<ETCD Performance>>

| ETCD is performing optimally 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/proxy-settings.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<OpenShift Proxy Settings>>

| OpenShift Proxy is not configured 

|{set:cellbgcolor:#A6B9BF}
Not Applicable


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/infra-taints.item

// Category
|
{set:cellbgcolor!}
Cluster Config

// Item Evaluated
a|
<<Infrastructure Taints>>

| 3 infrastructure nodes are not tainted 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# App Dev

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/application-probes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Application Probes>>

| Many user workloads are missing probes: 71.4% missing readiness probes, 42.9% missing liveness probes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/resource-quotas.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<Resource Quotas>>

| Only 0.0% of user namespaces (0 out of 8) have both resource quotas and limit ranges configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/emptydir-volumes.item

// Category
|
{set:cellbgcolor!}
Applications

// Item Evaluated
a|
<<EmptyDir Volumes>>

| 66.7% of user workloads (6 out of 9) are using emptyDir volumes 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/limit-range.item

// Category
|
{set:cellbgcolor!}
App Dev

// Item Evaluated
a|
<<LimitRange Configuration>>

| Only 12.5% of user namespaces (1 out of 8) have LimitRange configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Security

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/cluster-default-scc.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Security Context Constraint>>

| Default security context constraint (restricted) has not been modified 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/default-project-template.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Default Project Template>>

| Default project template is configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/self-provisioner.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Self Provisioner>>

| Self-provisioner role binding includes system:authenticated:oauth, allowing uncontrolled namespace creation 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/kubeadmin-user.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Kubeadmin User>>

| The kubeadmin user has been removed 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/identity-provider.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Identity Provider Configuration>>

| Identity providers are configured (HTPasswd), but no LDAP provider found 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-backup.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Backup>>

| No CronJobs found that might be backing up etcd 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/etcd-encryption.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<ETCD Encryption>>

| ETCD encryption is not enabled 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/elevated-privileges.item

// Category
|
{set:cellbgcolor!}
Security

// Item Evaluated
a|
<<Elevated Privileges>>

| No user workloads using privileged containers were found 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

# Op-Ready

[cols="1,2,2,3", options=header]
|===
|*Category*
|*Item Evaluated*
|*Observed Result*
|*Recommendation*

// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-stack-config.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Stack Configuration>>

| Monitoring stack configuration needs improvement: monitoring stack is missing configuration for components: metricsServer, node placement configuration incomplete for components: metricsServer, remote write storage not configured for long-term metrics retention 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarders-ops.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Forwarders OPS>>

| No external log forwarding is configured for operations logs 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-forwarder.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Log Forwarding>>

| Loki logging is configured but no external forwarding is set up 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-install.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Installation>>

| OpenShift Logging with Loki is installed and configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-health.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Health>>

| Loki Logging is healthy 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-placement.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Logging Component Placement>>

| All Loki pods are scheduled on infrastructure nodes 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/logging-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<OpenShift Logging Storage>>

| Loki storage schema needs to be updated 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/service-monitors.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Service Monitors>>

| Found 3 ServiceMonitors for application metrics monitoring 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/alerts-forwarding.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Alerts Forwarding>>

| No external alert forwarding configured 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/monitoring-storage.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<Monitoring Storage>>

| OpenShift monitoring components have persistent storage configured 

|{set:cellbgcolor:#00FF00}
No Change


// ------------------------ITEM END
// ------------------------ITEM START
// ----ITEM SOURCE:  ./content/healthcheck-items/user-workload-monitoring.item

// Category
|
{set:cellbgcolor!}
Op-Ready

// Item Evaluated
a|
<<User Workload Monitoring>>

| User Workload Monitoring is enabled but missing configuration for components: prometheusOperator, prometheus, thanosRuler, alertmanager 

|{set:cellbgcolor:#FEFE20}
Changes Recommended


// ------------------------ITEM END
|===

<<<

{set:cellbgcolor!}

== Ingress Controller Type

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

**Observation**

Default OpenShift ingress controller is in use

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Ingress Controller Placement

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "284384"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

**Observation**

Ingress controller node placement is not configured

**Recommendation**

Configure the ingress controller to be placed on infrastructure nodes

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#nw-ingress-controller-configuration-parameters_configuring-ingress

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Ingress Controller Replicas

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "284384"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

**Observation**

Ingress controller has insufficient replicas: 2 (recommended: >= 3)

**Recommendation**

Increase the number of ingress controller replicas to at least 3 for high availability

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#configuring-ingress

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Ingress Certificate

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "284384"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

**Observation**

Custom certificate is properly configured for the default ingress controller

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== CNI Network Plugin

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: Network
  metadata:
    creationTimestamp: "2025-04-14T17:52:03Z"
    generation: 5
    name: cluster
    resourceVersion: "286051"
    uid: 96403a0a-3abf-47a0-a692-a3e5407a1195
  spec:
    clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
    externalIP:
      policy: {}
    networkDiagnostics:
      mode: ""
      sourcePlacement: {}
      targetPlacement: {}
    networkType: OVNKubernetes
    serviceNetwork:
    - 172.30.0.0/16
  status:
    clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
    clusterNetworkMTU: 8901
    conditions:
    - lastTransitionTime: "2025-04-15T13:08:12Z"
      message: ""
      reason: AsExpected
      status: "True"
      type: NetworkDiagnosticsAvailable
    networkType: OVNKubernetes
    serviceNetwork:
    - 172.30.0.0/16
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

Cluster is using the recommended CNI network plugin: OVNKubernetes

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Network Policy

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
NAMESPACE          NAME                                       POD-SELECTOR                                               AGE
openshift-gitops   openshift-gitops-redis-ha-network-policy   app.kubernetes.io/name=openshift-gitops-redis-ha-haproxy   22h
openshift-gitops   openshift-gitops-redis-network-policy      app.kubernetes.io/name=openshift-gitops-redis              22h
showroom-user1     allow-from-all-namespaces                  <none>                                                     22h
showroom-user1     allow-from-ingress-namespace               <none>                                                     22h
user1-argocd       user1-argo-redis-ha-network-policy         app.kubernetes.io/name=user1-argo-redis-ha-haproxy         22h
user1-argocd       user1-argo-redis-network-policy            app.kubernetes.io/name=user1-argo-redis                    22h

----

**Observation**

Found 6 network policies in the cluster

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Ingress Controller

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
Issues:
Ingress controller node placement is not configured
Ingress controller has insufficient replicas: 2 (recommended: >= 3)

apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-04-14T17:56:22Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "284384"
  uid: 820c57c6-6038-479b-8a42-8ead3abfbdb9
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: cert-manager-ingress-cert
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2025-04-14T17:56:25Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2025-04-14T17:56:52Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-15T13:06:51Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:22Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2025-04-14T18:04:07Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-smd5h.smd5h.sandbox425.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
  tlsProfile:
    ciphers:
    - ECDHE-ECDSA-AES128-GCM-SHA256
    - ECDHE-RSA-AES128-GCM-SHA256
    - ECDHE-ECDSA-AES256-GCM-SHA384
    - ECDHE-RSA-AES256-GCM-SHA384
    - ECDHE-ECDSA-CHACHA20-POLY1305
    - ECDHE-RSA-CHACHA20-POLY1305
    - DHE-RSA-AES128-GCM-SHA256
    - DHE-RSA-AES256-GCM-SHA384
    - TLS_AES_128_GCM_SHA256
    - TLS_AES_256_GCM_SHA384
    - TLS_CHACHA20_POLY1305_SHA256
    minTLSVersion: VersionTLS12

----

**Observation**

Ingress controller has 2 configuration issues

**Recommendation**

Configure the ingress controller to be placed on infrastructure nodes

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#nw-ingress-controller-configuration-parameters_configuring-ingress

Increase the number of ingress controller replicas to at least 3 for high availability

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/networking/index#configuring-ingress

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Storage Classes

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
NAME                          PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2-csi                       ebs.csi.aws.com                         Delete          WaitForFirstConsumer   true                   23h
gp3-csi (default)             ebs.csi.aws.com                         Delete          WaitForFirstConsumer   true                   23h
ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   22h
ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   22h
openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         Delete          Immediate              false                  22h

----

**Observation**

Default storage class 'gp3-csi' is configured and RWX-capable storage is available

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Persistent Volumes

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                           STORAGECLASS                  VOLUMEATTRIBUTESCLASS   REASON   AGE     VOLUMEMODE
pvc-042f592c-6344-4c51-a130-d9a15d103e7b   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-gp3-csi-2-data-0rgwwv           gp3-csi                       <unset>                          22h     Block
pvc-0b70e9cf-722e-4055-bf1a-7b178597480d   10Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-ingester-0                  gp3-csi                       <unset>                          22h     Filesystem
pvc-0e7468f7-9013-4781-a378-e7504c88de8b   10Gi       RWO            Delete           Bound    openshift-monitoring/alertmanager-main-db-alertmanager-main-1   gp3-csi                       <unset>                          6h48m   Filesystem
pvc-15046389-45e4-43ef-a934-6d8191aa4ac0   50Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-c                               gp3-csi                       <unset>                          22h     Filesystem
pvc-312588a4-bb36-4ce8-b0ce-f048944c1606   50Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-index-gateway-1             gp3-csi                       <unset>                          22h     Filesystem
pvc-3eb073c6-ff9a-4b65-abff-6eb3acc2431c   50Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-a                               gp3-csi                       <unset>                          22h     Filesystem
pvc-4cf63059-2db0-4c4b-9988-32b928c9edcc   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-gp3-csi-0-data-0k2v72           gp3-csi                       <unset>                          22h     Block
pvc-50ef79db-6124-485a-b50d-591930fb8a4f   10Gi       RWO            Delete           Bound    openshift-monitoring/prometheus-k8s-db-prometheus-k8s-1         gp3-csi                       <unset>                          6h59m   Filesystem
pvc-54d7c6e5-8f03-4c33-8c3b-7946ebe63cda   10Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-ingester-1                  gp3-csi                       <unset>                          22h     Filesystem
pvc-56728660-2fd0-41e0-8f99-04f21131d0fb   50Gi       RWO            Delete           Bound    openshift-storage/db-noobaa-db-pg-0                             ocs-storagecluster-ceph-rbd   <unset>                          22h     Filesystem
pvc-5891c087-769f-426f-a925-be04857ef3eb   10Gi       RWO            Delete           Bound    openshift-monitoring/alertmanager-main-db-alertmanager-main-0   gp3-csi                       <unset>                          6h48m   Filesystem
pvc-6cc8583c-16ad-4301-9fca-8585886f4a06   10Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-compactor-0                 gp3-csi                       <unset>                          22h     Filesystem
pvc-87b3816d-4712-4e21-915e-229ab4c35a6d   150Gi      RWO            Delete           Bound    openshift-logging/wal-lokistack-ingester-1                      gp3-csi                       <unset>                          22h     Filesystem
pvc-93951a27-b38b-48ef-9f77-27d1d2ae3ba0   50Gi       RWO            Delete           Bound    openshift-logging/storage-lokistack-index-gateway-0             gp3-csi                       <unset>                          22h     Filesystem
pvc-a34c3fdb-8967-441e-ac6e-3a50559c074b   10Gi       RWO            Delete           Bound    openshift-monitoring/prometheus-k8s-db-prometheus-k8s-0         gp3-csi                       <unset>                          6h59m   Filesystem
pvc-b4bf6f83-f98e-4d29-ad8b-c9363e0e0830   5Gi        RWO            Delete           Bound    showroom-user1/showroom-terminal-lab-user-home                  gp3-csi                       <unset>                          22h     Filesystem
pvc-dad670ba-3445-46e2-b45d-11a336509baf   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-gp3-csi-1-data-0jmbq7           gp3-csi                       <unset>                          22h     Block
pvc-e1b9e364-f77d-43d5-9e22-df1a8a4063c4   50Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-b                               gp3-csi                       <unset>                          22h     Filesystem
pvc-e29df30a-e702-4502-b50f-364b9c98d3b2   150Gi      RWO            Delete           Bound    openshift-logging/wal-lokistack-ingester-0                      gp3-csi                       <unset>                          22h     Filesystem

----

**Observation**

All 19 persistent volumes are healthy

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Storage Performance

[cols="^"]
|===
|
{set:cellbgcolor:#80E5FF}
Advisory
|===

[source, bash]
----
Storage Class Details:
apiVersion: v1
items:
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    creationTimestamp: "2025-04-14T17:56:07Z"
    name: gp2-csi
    resourceVersion: "5893"
    uid: 0f5e301f-5847-4885-b03c-e88301aa68a1
  parameters:
    encrypted: "true"
    type: gp2
  provisioner: ebs.csi.aws.com
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      storageclass.kubernetes.io/is-default-class: "true"
    creationTimestamp: "2025-04-14T17:56:07Z"
    name: gp3-csi
    resourceVersion: "5888"
    uid: 128fdee7-f25c-4de2-b6a7-d6e7d1772021
  parameters:
    encrypted: "true"
    type: gp3
  provisioner: ebs.csi.aws.com
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides RWO Filesystem volumes, and RWO and RWX Block volumes
    creationTimestamp: "2025-04-14T18:44:57Z"
    name: ocs-storagecluster-ceph-rbd
    resourceVersion: "50068"
    uid: eb032e4e-ee68-4148-bd84-1e6b1cfdb4d4
  parameters:
    clusterID: openshift-storage
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
    csi.storage.k8s.io/fstype: ext4
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
    imageFeatures: layering,deep-flatten,exclusive-lock,object-map,fast-diff
    imageFormat: "2"
    pool: ocs-storagecluster-cephblockpool
  provisioner: openshift-storage.rbd.csi.ceph.com
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides RWO and RWX Filesystem volumes
    creationTimestamp: "2025-04-14T18:45:13Z"
    name: ocs-storagecluster-cephfs
    resourceVersion: "50388"
    uid: 3763c8da-be52-44ba-a936-536f75d43c70
  parameters:
    clusterID: openshift-storage
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
    fsName: ocs-storagecluster-cephfilesystem
  provisioner: openshift-storage.cephfs.csi.ceph.com
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
- apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      description: Provides Object Bucket Claims (OBCs)
    creationTimestamp: "2025-04-14T18:46:07Z"
    name: openshift-storage.noobaa.io
    resourceVersion: "51326"
    uid: 7bd0b503-458c-472c-afd7-748b4a27e3e6
  parameters:
    bucketclass: noobaa-default-bucket-class
  provisioner: openshift-storage.noobaa.io/obc
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

No storage classes with explicit performance characteristics found

**Recommendation**

Consider defining storage classes with different performance tiers

Label storage classes with performance characteristics for better workload placement

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Provider

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
Infrastructure Name: cluster-smd5h-x4xzc
Platform Type: AWS

AWS Region: eu-central-1
Control Plane Topology: HighlyAvailable
Infrastructure Topology: HighlyAvailable



apiVersion: config.openshift.io/v1
kind: Infrastructure
metadata:
  creationTimestamp: "2025-04-14T17:52:02Z"
  generation: 1
  name: cluster
  resourceVersion: "434"
  uid: d1c4bffc-05bb-4420-9e23-d853d09cade8
spec:
  cloudConfig:
    key: config
    name: cloud-provider-config
  platformSpec:
    aws: {}
    type: AWS
status:
  apiServerInternalURI: https://api-int.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  apiServerURL: https://api.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  controlPlaneTopology: HighlyAvailable
  cpuPartitioning: None
  etcdDiscoveryDomain: ""
  infrastructureName: cluster-smd5h-x4xzc
  infrastructureTopology: HighlyAvailable
  platform: AWS
  platformStatus:
    aws:
      region: eu-central-1
    type: AWS

----

**Observation**

Infrastructure provider type: AWS

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Installation Type

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
Infrastructure Name: cluster-smd5h-x4xzc
Platform Type: AWS

Installation Type: Installer-Provisioned Infrastructure (IPI)

This is an IPI installation where the OpenShift installer provisioned the infrastructure automatically.
Control Plane Topology: HighlyAvailable
Infrastructure Topology: HighlyAvailable

apiVersion: config.openshift.io/v1
kind: Infrastructure
metadata:
  creationTimestamp: "2025-04-14T17:52:02Z"
  generation: 1
  name: cluster
  resourceVersion: "434"
  uid: d1c4bffc-05bb-4420-9e23-d853d09cade8
spec:
  cloudConfig:
    key: config
    name: cloud-provider-config
  platformSpec:
    aws: {}
    type: AWS
status:
  apiServerInternalURI: https://api-int.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  apiServerURL: https://api.cluster-smd5h.smd5h.sandbox425.opentlc.com:6443
  controlPlaneTopology: HighlyAvailable
  cpuPartitioning: None
  etcdDiscoveryDomain: ""
  infrastructureName: cluster-smd5h-x4xzc
  infrastructureTopology: HighlyAvailable
  platform: AWS
  platformStatus:
    aws:
      region: eu-central-1
    type: AWS

----

**Observation**

Installation type: IPI

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== UPI with MachineSets

[cols="^"]
|===
|
{set:cellbgcolor:#A6B9BF}
Not Applicable
|===

**Observation**

This is an IPI installation, UPI MachineSets check is not applicable

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Node Status

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker           23h   v1.29.14+7cf4c05
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   23h   v1.29.14+7cf4c05
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker           23h   v1.29.14+7cf4c05
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   23h   v1.29.14+7cf4c05
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   23h   v1.29.14+7cf4c05
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker           23h   v1.29.14+7cf4c05

----

**Observation**

All 6 nodes are ready

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Node Usage

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
NAME                                           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-10-0-14-186.eu-central-1.compute.internal   497m         3%     9358Mi          15%       
ip-10-0-2-3.eu-central-1.compute.internal      489m         6%     7905Mi          54%       
ip-10-0-20-141.eu-central-1.compute.internal   492m         3%     8681Mi          13%       
ip-10-0-31-43.eu-central-1.compute.internal    338m         4%     10384Mi         71%       
ip-10-0-58-101.eu-central-1.compute.internal   383m         5%     10872Mi         74%       
ip-10-0-8-87.eu-central-1.compute.internal     289m         1%     4968Mi          7%        

----

**Observation**

All nodes are within resource usage thresholds

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Cluster Version

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: ClusterVersion
  metadata:
    creationTimestamp: "2025-04-14T17:52:08Z"
    generation: 2
    name: version
    resourceVersion: "288484"
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  spec:
    channel: stable-4.16
    clusterID: 9c3defd1-585f-489f-9dbc-3995377cf250
  status:
    availableUpdates: null
    capabilities:
      enabledCapabilities:
      - Build
      - CSISnapshot
      - CloudControllerManager
      - CloudCredential
      - Console
      - DeploymentConfig
      - ImageRegistry
      - Ingress
      - Insights
      - MachineAPI
      - NodeTuning
      - OperatorLifecycleManager
      - Storage
      - baremetal
      - marketplace
      - openshift-samples
      knownCapabilities:
      - Build
      - CSISnapshot
      - CloudControllerManager
      - CloudCredential
      - Console
      - DeploymentConfig
      - ImageRegistry
      - Ingress
      - Insights
      - MachineAPI
      - NodeTuning
      - OperatorLifecycleManager
      - Storage
      - baremetal
      - marketplace
      - openshift-samples
    conditions:
    - lastTransitionTime: "2025-04-14T17:52:30Z"
      message: Capabilities match configured spec
      reason: AsExpected
      status: "False"
      type: ImplicitlyEnabledCapabilities
    - lastTransitionTime: "2025-04-14T17:52:30Z"
      message: Payload loaded version="4.16.38" image="quay.io/openshift-release-dev/ocp-release@sha256:6da09834a9e0e30a79f77c13c2520a25172d8be3fc044dc2ad1392d69b2edfbf"
        architecture="amd64"
      reason: PayloadLoaded
      status: "True"
      type: ReleaseAccepted
    - lastTransitionTime: "2025-04-14T18:13:18Z"
      message: Done applying 4.16.38
      status: "True"
      type: Available
    - lastTransitionTime: "2025-04-15T13:10:35Z"
      status: "False"
      type: Failing
    - lastTransitionTime: "2025-04-14T18:13:18Z"
      message: Cluster version is 4.16.38
      status: "False"
      type: Progressing
    - lastTransitionTime: "2025-04-14T17:52:30Z"
      status: "True"
      type: RetrievedUpdates
    desired:
      channels:
      - candidate-4.16
      - candidate-4.17
      - candidate-4.18
      - eus-4.16
      - eus-4.18
      - fast-4.16
      - fast-4.17
      - fast-4.18
      - stable-4.16
      - stable-4.17
      - stable-4.18
      image: quay.io/openshift-release-dev/ocp-release@sha256:6da09834a9e0e30a79f77c13c2520a25172d8be3fc044dc2ad1392d69b2edfbf
      url: https://access.redhat.com/errata/RHSA-2025:3301
      version: 4.16.38
    history:
    - completionTime: "2025-04-14T18:13:18Z"
      image: quay.io/openshift-release-dev/ocp-release@sha256:6da09834a9e0e30a79f77c13c2520a25172d8be3fc044dc2ad1392d69b2edfbf
      startedTime: "2025-04-14T17:52:30Z"
      state: Completed
      verified: false
      version: 4.16.38
    observedGeneration: 2
    versionHash: FzelQgAxbzc=
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

Cluster version 4.16.38 is up to date

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Cluster Operators

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
authentication                             4.16.38   True        False         False      4h14m   
baremetal                                  4.16.38   True        False         False      23h     
cloud-controller-manager                   4.16.38   True        False         False      23h     
cloud-credential                           4.16.38   True        False         False      23h     
cluster-autoscaler                         4.16.38   True        False         False      23h     
config-operator                            4.16.38   True        False         False      23h     
console                                    4.16.38   True        False         False      22h     
control-plane-machine-set                  4.16.38   True        False         False      23h     
csi-snapshot-controller                    4.16.38   True        False         False      12h     
dns                                        4.16.38   True        False         False      4h14m   
etcd                                       4.16.38   True        False         False      23h     
image-registry                             4.16.38   True        False         False      23h     
ingress                                    4.16.38   True        False         False      4h13m   
insights                                   4.16.38   True        False         False      23h     
kube-apiserver                             4.16.38   True        False         False      23h     
kube-controller-manager                    4.16.38   True        False         False      23h     
kube-scheduler                             4.16.38   True        False         False      23h     
kube-storage-version-migrator              4.16.38   True        False         False      23h     
machine-api                                4.16.38   True        False         False      23h     
machine-approver                           4.16.38   True        False         False      23h     
machine-config                             4.16.38   True        False         False      23h     
marketplace                                4.16.38   True        False         False      23h     
monitoring                                 4.16.38   True        False         False      23h     
network                                    4.16.38   True        False         False      23h     
node-tuning                                4.16.38   True        False         False      23h     
openshift-apiserver                        4.16.38   True        False         False      4h14m   
openshift-controller-manager               4.16.38   True        False         False      4h14m   
openshift-samples                          4.16.38   True        False         False      23h     
operator-lifecycle-manager                 4.16.38   True        False         False      23h     
operator-lifecycle-manager-catalog         4.16.38   True        False         False      23h     
operator-lifecycle-manager-packageserver   4.16.38   True        False         False      12h     
service-ca                                 4.16.38   True        False         False      23h     
storage                                    4.16.38   True        False         False      12h     

----

**Observation**

All cluster operators are available

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Control Plane Node Schedulability

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   23h   v1.29.14+7cf4c05   10.0.2.3      <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   23h   v1.29.14+7cf4c05   10.0.31.43    <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   23h   v1.29.14+7cf4c05   10.0.58.101   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

**Observation**

All control plane nodes are properly configured to prevent regular workloads

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Nodes

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
NAME                                           STATUS   ROLES          AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

**Observation**

Found 3 infrastructure nodes but not all are properly tainted

**Recommendation**

Add appropriate taints to infrastructure nodes to prevent regular workloads from being scheduled on them

Use 'oc adm taint nodes <node-name> node-role.kubernetes.io/infra=:NoSchedule'

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Workloads on Infrastructure Nodes

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
User workloads running on infrastructure nodes:
- Pod 'cert-manager-c965b6b46-qjhgn' in namespace 'cert-manager' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'cert-manager-cainjector-7b97bcbf6-8slg5' in namespace 'cert-manager' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'cert-manager-webhook-6f8fdf478f-gvcmp' in namespace 'cert-manager' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'cert-manager-operator-controller-manager-79d54c6cf4-474d5' in namespace 'cert-manager-operator' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'showroom-775d8f49dc-4c2kk' in namespace 'showroom-user1' is running on infrastructure node 'ip-10-0-8-87.eu-central-1.compute.internal'
- Pod 'user1-argo-application-controller-0' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'user1-argo-dex-server-64bcdc785b-pwb8r' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-14-186.eu-central-1.compute.internal'
- Pod 'user1-argo-redis-fb879dcd7-9hc5k' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'user1-argo-repo-server-55c4b88b58-njk4d' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-20-141.eu-central-1.compute.internal'
- Pod 'user1-argo-server-69f4cbdd44-jcl6p' in namespace 'user1-argocd' is running on infrastructure node 'ip-10-0-14-186.eu-central-1.compute.internal'

Infrastructure nodes:
NAME                                           STATUS   ROLES          AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

**Observation**

10 user workloads are running on infrastructure nodes

**Recommendation**

Infrastructure nodes should be dedicated to infrastructure components

Add taints to infrastructure nodes to prevent user workloads from running on them

Consider moving these workloads to worker nodes

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Node Schedule

[cols="^"]
|===
|
{set:cellbgcolor:#80E5FF}
Advisory
|===

[source, bash]
----
NAME                                           STATUS   ROLES                  AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker           23h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-2-3.eu-central-1.compute.internal      Ready    control-plane,master   23h   v1.29.14+7cf4c05   10.0.2.3      <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker           23h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-31-43.eu-central-1.compute.internal    Ready    control-plane,master   23h   v1.29.14+7cf4c05   10.0.31.43    <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-58-101.eu-central-1.compute.internal   Ready    control-plane,master   23h   v1.29.14+7cf4c05   10.0.58.101   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker           23h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

**Observation**

No custom namespace node selectors are configured

**Recommendation**

Consider configuring namespace node selectors to control workload placement

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/nodes/index#nodes-scheduler-node-selectors

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Machine Config Pool

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-f27053fdb325fead0ea67a44d88cc37f   True      False      False      3              3                   3                     0                      23h
worker   rendered-worker-a5adabfd0c4b230acaad384a895530fd   True      False      False      3              3                   3                     0                      23h

----

**Observation**

No dedicated infrastructure machine config pool found

**Recommendation**

Create a dedicated infrastructure machine config pool

In a production deployment, it is recommended that you deploy at least three machine sets to hold infrastructure components

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/machine_management/index#creating-infrastructure-machinesets

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Kubelet Garbage Collection

[cols="^"]
|===
|
{set:cellbgcolor:#80E5FF}
Advisory
|===

[source, bash]
----
No custom kubelet configuration found

Machine Config Pools:
master worker
----

**Observation**

No custom kubelet garbage collection configuration found (using defaults)

**Recommendation**

Consider configuring kubelet garbage collection parameters for production environments

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/nodes/index#nodes-nodes-garbage-collection

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Health

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
ETCD Operator Information:
apiVersion: config.openshift.io/v1
kind: ClusterOperator
metadata:
  annotations:
    exclude.release.openshift.io/internal-openshift-hosted: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
  creationTimestamp: "2025-04-14T17:52:30Z"
  generation: 1
  name: etcd
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    controller: true
    kind: ClusterVersion
    name: version
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  resourceVersion: "279367"
  uid: 1b210b7c-d7cf-4c00-b837-261015962602
spec: {}
status:
  conditions:
  - lastTransitionTime: "2025-04-14T18:01:02Z"
    message: |-
      NodeControllerDegraded: All master nodes are ready
      EtcdMembersDegraded: No unhealthy members found
    reason: AsExpected
    status: "False"
    type: Degraded
  - lastTransitionTime: "2025-04-14T18:13:56Z"
    message: |-
      NodeInstallerProgressing: 3 nodes are at revision 8
      EtcdMembersProgressing: No unstarted etcd members found
    reason: AsExpected
    status: "False"
    type: Progressing
  - lastTransitionTime: "2025-04-14T17:57:47Z"
    message: |-
      StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8
      EtcdMembersAvailable: 3 members are available
    reason: AsExpected
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-14T17:56:00Z"
    message: All is well
    reason: AsExpected
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2025-04-14T17:56:00Z"
    reason: NoData
    status: Unknown
    type: EvaluationConditionsDetected
  extension: null
  relatedObjects:
  - group: operator.openshift.io
    name: cluster
    resource: etcds
  - group: ""
    name: openshift-config
    resource: namespaces
  - group: ""
    name: openshift-config-managed
    resource: namespaces
  - group: ""
    name: openshift-etcd-operator
    resource: namespaces
  - group: ""
    name: openshift-etcd
    resource: namespaces
  versions:
  - name: raw-internal
    version: 4.16.38
  - name: etcd
    version: 4.16.38
  - name: operator
    version: 4.16.38


ETCD Pods Information:
NAME                                                READY   STATUS    RESTARTS   AGE
etcd-ip-10-0-2-3.eu-central-1.compute.internal      4/4     Running   8          23h
etcd-ip-10-0-31-43.eu-central-1.compute.internal    4/4     Running   8          23h
etcd-ip-10-0-58-101.eu-central-1.compute.internal   4/4     Running   8          23h


ETCD Performance Information:
=== ETCD Performance Metrics ===

No 'took too long' messages found (good).

No heartbeat issues found (good).

No clock drift issues found (good).

Could not run etcd performance diagnostics. This is non-critical.

ETCD Pod Resources:
Node ip-10-0-2-3.eu-central-1.compute.internal resource usage:
NAME                                        CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-10-0-2-3.eu-central-1.compute.internal   523m         6%     7893Mi          54%       

Node ip-10-0-31-43.eu-central-1.compute.internal resource usage:
NAME                                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-10-0-31-43.eu-central-1.compute.internal   928m         12%    10337Mi         70%       

Node ip-10-0-58-101.eu-central-1.compute.internal resource usage:
NAME                                           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-10-0-58-101.eu-central-1.compute.internal   409m         5%     10818Mi         74%       


=== ETCD Performance Best Practices ===
For optimal etcd performance:
1. Use fast SSD or NVMe storage dedicated to etcd
2. Ensure 99th percentile of fsync is below 10ms
3. Network latency between etcd nodes should be below 2ms
4. CPU should not be overcommitted on etcd nodes
5. Sequential fsync IOPS should be at least 500 for medium/large clusters


----

**Observation**

ETCD cluster is healthy and performing well

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Performance

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
=== ETCD Performance Analysis ===

== Compaction Performance ==
Compaction performance is good. No slow compactions detected.


== Entry Apply Performance ==
Entry apply performance is good. No delays detected.


== Heartbeat Performance ==
Heartbeat performance is good. No issues detected.


== Clock Synchronization ==
Clock synchronization appears good. No drift issues detected.


== Node Resource Usage ==
Node resource usage appears to be within normal ranges.


== ETCD Performance Recommendations ==
For optimal etcd performance, the following are recommended:

1. Storage requirements:
   - Use dedicated SSDs (preferably NVMe) for etcd
   - For cloud environments, use high-performance storage:
     * AWS: io1/io2/io2-block-express with at least 2000 IOPS
     * Azure: Premium SSD or Ultra Disk
     * GCP: SSD Persistent Disk
   - For on-premise: avoid shared storage, use local NVMe or high-performance SSD
   - Avoid VM snapshots which can severely impact I/O performance

2. Performance metrics to aim for:
   - WAL fsync 99th percentile: < 10ms (ideally < 5ms)
   - Sequential fsync IOPS:
     * Small clusters: at least 50 IOPS
     * Medium clusters: at least 300 IOPS
     * Large clusters: at least 500 IOPS
     * Heavy workloads: at least 800 IOPS

3. Node configuration:
   - Ensure sufficient CPU resources (avoid overcommitment)
   - Configure proper time synchronization with chrony
   - Network latency between etcd nodes should be < 2ms
   - Avoid placing etcd nodes across different data centers

4. Maintenance practices:
   - Regular defragmentation during maintenance windows
   - Clean up unused projects, secrets, deployments, and other resources
   - Evaluate the impact of installing new operators



----

**Observation**

ETCD is performing optimally

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Proxy Settings

[cols="^"]
|===
|
{set:cellbgcolor:#A6B9BF}
Not Applicable
|===

[source, bash]
----
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  creationTimestamp: "2025-04-14T17:52:04Z"
  generation: 1
  name: cluster
  resourceVersion: "531"
  uid: 9b46f109-6038-47c6-b68d-154f25aa7ef4
spec:
  trustedCA:
    name: ""
status: {}

----

**Observation**

OpenShift Proxy is not configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Infrastructure Taints

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
Nodes without taints:
ip-10-0-14-186.eu-central-1.compute.internal
ip-10-0-20-141.eu-central-1.compute.internal
ip-10-0-8-87.eu-central-1.compute.internal

Taint information:
ip-10-0-14-186.eu-central-1.compute.internal: 
ip-10-0-20-141.eu-central-1.compute.internal: 
ip-10-0-8-87.eu-central-1.compute.internal: 


Nodes:
NAME                                           STATUS   ROLES          AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                                KERNEL-VERSION                 CONTAINER-RUNTIME
ip-10-0-14-186.eu-central-1.compute.internal   Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.14.186   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-20-141.eu-central-1.compute.internal   Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.20.141   <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9
ip-10-0-8-87.eu-central-1.compute.internal     Ready    infra,worker   23h   v1.29.14+7cf4c05   10.0.8.87     <none>        Red Hat Enterprise Linux CoreOS 416.94.202503252048-0   5.14.0-427.62.1.el9_4.x86_64   cri-o://1.29.13-4.rhaos4.16.git3505178.el9

----

**Observation**

3 infrastructure nodes are not tainted

**Recommendation**

Add taints to infrastructure nodes to prevent regular workloads from being scheduled on them

Use 'oc adm taint nodes <node-name> node-role.kubernetes.io/infra=:NoSchedule'

Refer to the documentation at https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/nodes/scheduling-pods-to-specific-nodes

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Application Probes

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
Summary:
- Total user workloads: 7
- Workloads missing readiness probes: 5 (71.4%)
- Workloads missing liveness probes: 3 (42.9%)
- Workloads missing both probes: 3 (42.9%)

Affected namespaces:
- cert-manager
- showroom-user1
- user1-argocd

Affected workloads:
- Deployment 'cert-manager' in namespace 'cert-manager' is missing readiness probe
- Deployment 'cert-manager-cainjector' in namespace 'cert-manager' is missing both readiness and liveness probes
- Deployment 'showroom' in namespace 'showroom-user1' is missing both readiness and liveness probes
- Deployment 'user1-argo-dex-server' in namespace 'user1-argocd' is missing readiness probe
- Deployment 'user1-argo-redis' in namespace 'user1-argocd' is missing both readiness and liveness probes


What are Readiness and Liveness Probes?

Readiness Probe: Determines if a container is ready to accept traffic. When a pod's readiness check fails, it is removed from service load balancers.

Liveness Probe: Determines if a container is still running as expected. When a liveness check fails, Kubernetes will restart the container.

Benefits of using probes:
- Prevents traffic from being sent to unready containers
- Automatically restarts unhealthy containers
- Improves application resilience and availability
- Facilitates smoother deployments and updates
- Provides better visibility into application health

----

**Observation**

Many user workloads are missing probes: 71.4% missing readiness probes, 42.9% missing liveness probes

**Recommendation**

Configure readiness and liveness probes for all user workloads

Follow the Kubernetes documentation on pod lifecycle and probes: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Resource Quotas

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
Summary:
- Total user namespaces: 8
- Namespaces with resource quotas: 0 (0.0%)
- Namespaces with limit ranges: 1 (12.5%)
- Namespaces with both: 0 (0.0%)

Namespaces without resource quotas:
- cert-manager
- cert-manager-operator
- showroom-user1
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo

Namespaces without limit ranges:
- cert-manager
- cert-manager-operator
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo

Namespaces without both:
- cert-manager
- cert-manager-operator
- showroom-user1
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo


What are Resource Quotas and Limit Ranges?

Resource Quotas: Define the total amount of resources a namespace can use. They limit the total CPU, memory, and other resources that can be consumed by all pods in a namespace.

Limit Ranges: Define default resource limits and requests for containers in a namespace. They can also enforce minimum and maximum resource usage limits.

Benefits of using Resource Quotas and Limit Ranges:
- Prevent resource starvation by limiting the total resources a namespace can consume
- Ensure fair resource allocation across namespaces
- Protect against runaway applications that might consume all available resources
- Enforce resource constraints and prevent resource leaks
- Help with capacity planning and cost management

----

**Observation**

Only 0.0% of user namespaces (0 out of 8) have both resource quotas and limit ranges configured

**Recommendation**

Configure resource quotas and limit ranges for all user namespaces

Follow the Kubernetes documentation on resource quotas: https://kubernetes.io/docs/concepts/policy/resource-quotas/

Follow the Kubernetes documentation on limit ranges: https://kubernetes.io/docs/concepts/policy/limit-range/

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== EmptyDir Volumes

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
Summary:
- Total user workloads: 9
- Workloads using emptyDir volumes: 6 (66.7%)

Affected namespaces:
- showroom-user1
- user1-argocd

Affected workloads:
- Pod 'showroom-775d8f49dc-4c2kk' in namespace 'showroom-user1' is using emptyDir volume
- Deployment 'showroom' in namespace 'showroom-user1' is using emptyDir volume
- Pod 'user1-argo-dex-server-64bcdc785b-pwb8r' in namespace 'user1-argocd' is using emptyDir volume
- Pod 'user1-argo-repo-server-55c4b88b58-njk4d' in namespace 'user1-argocd' is using emptyDir volume
- Deployment 'user1-argo-dex-server' in namespace 'user1-argocd' is using emptyDir volume
- Deployment 'user1-argo-repo-server' in namespace 'user1-argocd' is using emptyDir volume


What are emptyDir Volumes?

An emptyDir volume is created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.

Risks of using emptyDir volumes:
- Data loss: All data is lost when the pod is deleted or rescheduled
- No persistence across pod restarts or rescheduling
- Not suitable for stateful applications that need data persistence
- No data sharing between different pods or nodes

Recommended alternatives:
- PersistentVolumeClaims (PVCs) for persistent storage
- ConfigMaps or Secrets for configuration data
- External storage services for important data

----

**Observation**

66.7% of user workloads (6 out of 9) are using emptyDir volumes

**Recommendation**

Use persistent volumes instead of emptyDir for data that needs to persist

Review existing workloads using emptyDir to ensure they don't store important data

Follow the Kubernetes documentation on volumes: https://kubernetes.io/docs/concepts/storage/volumes/

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== LimitRange Configuration

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
Namespaces without LimitRange:
- cert-manager
- cert-manager-operator
- user1-argocd
- user1-bgd
- user1-bgdh
- user1-bgdk
- user1-todo

NAMESPACE        NAME                                  CREATED AT
showroom-user1   showroom-user1-core-resource-limits   2025-04-14T18:35:21Z

----

**Observation**

Only 12.5% of user namespaces (1 out of 8) have LimitRange configured

**Recommendation**

Configure LimitRange resources in all namespaces to control resource usage

Set up a default project template including LimitRange

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Security Context Constraint

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
groups: []
kind: SecurityContextConstraints
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    kubernetes.io/description: restricted denies access to all host features and requires
      pods to be run with a UID, and SELinux context that are allocated to the namespace.
    release.openshift.io/create-only: "true"
  creationTimestamp: "2025-04-14T17:51:48Z"
  generation: 1
  name: restricted
  resourceVersion: "345"
  uid: 7b3af7ab-1ac5-4d4a-90b1-7875dcbe0327
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- KILL
- MKNOD
- SETUID
- SETGID
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- csi
- downwardAPI
- emptyDir
- ephemeral
- persistentVolumeClaim
- projected
- secret

----

**Observation**

Default security context constraint (restricted) has not been modified

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Default Project Template

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
apiVersion: config.openshift.io/v1
kind: Project
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2025-04-14T17:52:49Z"
  generation: 2
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  resourceVersion: "27474"
  uid: bd1f348f-6c9e-4222-a449-f3ba1a32d57a
spec:
  projectRequestMessage: To provision Projects you must request access in https://labs.opentlc.com
    or https://rhpds.redhat.com.
  projectRequestTemplate:
    name: project-request

----

**Observation**

Default project template is configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Self Provisioner

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2025-04-14T17:58:08Z"
  name: self-provisioners
  resourceVersion: "10732"
  uid: aa2f8fe3-9217-41d0-9c88-9bf93d411015
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: self-provisioner
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:authenticated:oauth

----

**Observation**

Self-provisioner role binding includes system:authenticated:oauth, allowing uncontrolled namespace creation

**Recommendation**

Remove the self-provisioner role from the system:authenticated:oauth group

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/building_applications/index#disabling-project-self-provisioning_configuring-project-creation

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Kubeadmin User

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
Secret 'kubeadmin' not found in 'kube-system' namespace
----

**Observation**

The kubeadmin user has been removed

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Identity Provider Configuration

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2025-04-14T17:52:47Z"
  generation: 2
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
  resourceVersion: "27389"
  uid: 69381815-0b0e-4e04-9361-e15e739c5bfe
spec:
  identityProviders:
  - htpasswd:
      fileData:
        name: htpasswd
    mappingMethod: claim
    name: htpasswd_provider
    type: HTPasswd

----

**Observation**

Identity providers are configured (HTPasswd), but no LDAP provider found

**Recommendation**

Configure a central identity provider (LDAP) for better integration with existing identity management systems

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/authentication_and_authorization/index#configuring-ldap-identity-provider

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Backup

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
ETCD Cluster Operator status:
NAME   VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
etcd   4.16.38   True        False         False      23h     

----

**Observation**

No CronJobs found that might be backing up etcd

**Recommendation**

Set up regular etcd backups to protect against data loss

Follow the documentation at https://docs.openshift.com/container-platform/latest/backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.html

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== ETCD Encryption

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: APIServer
  metadata:
    annotations:
      include.release.openshift.io/ibm-cloud-managed: "true"
      include.release.openshift.io/self-managed-high-availability: "true"
      oauth-apiserver.openshift.io/secure-token-storage: "true"
      release.openshift.io/create-only: "true"
    creationTimestamp: "2025-04-14T17:52:31Z"
    generation: 1
    name: cluster
    ownerReferences:
    - apiVersion: config.openshift.io/v1
      kind: ClusterVersion
      name: version
      uid: e1b9e7fe-2578-4b28-9a34-5b3389ee9fb3
    resourceVersion: "767"
    uid: fd6ddf91-b4b6-434a-aa27-135c36d4914b
  spec:
    audit:
      profile: Default
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

ETCD encryption is not enabled

**Recommendation**

Enable etcd encryption to protect sensitive data

Follow the documentation at https://docs.openshift.com/container-platform/latest/security/encrypting-etcd.html

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Elevated Privileges

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

**Observation**

No user workloads using privileged containers were found

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Monitoring Stack Configuration

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
== Monitoring Stack Configuration Analysis ==

Cluster Monitoring Config exists:

[source, yaml]
----
apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"config.yaml":"enableUserWorkload: true\nprometheusOperator:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nnodeExporter:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 150Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\nprometheusK8s:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 3Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  retention: 7d\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 40Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nalertmanagerMain:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 10Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nmonitoringPlugin:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nopenshiftStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\ntelemeterClient:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nk8sPrometheusAdapter:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nthanosQuerier:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nprometheusOperatorAdmissionWebhook:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 100Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"cluster-monitoring-config","namespace":"openshift-monitoring"}}
  creationTimestamp: "2025-04-15T10:31:14Z"
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  resourceVersion: "328571"
  uid: 11f1e0ef-c0da-4948-99d0-4425e5996ebe

----

== Monitoring Stack Components ==

Using OpenShift 4.16 monitoring component requirements

Components explicitly configured in cluster-monitoring-config:
- prometheusOperator
- prometheusK8s
- alertmanagerMain
- thanosQuerier
- kubeStateMetrics
- monitoringPlugin
- openshiftStateMetrics
- telemeterClient
- nodeExporter
- prometheusOperatorAdmissionWebhook

WARNING: The following core components are NOT explicitly configured in cluster-monitoring-config:
- metricsServer

These components are running with default settings, which may not be optimal for your environment.
Consider configuring these components explicitly for better control over resources and behavior.

User Workload Monitoring enabled: true

Verifying component status in the cluster:
- prometheusOperator:  Running
- prometheusK8s:  Running
- alertmanagerMain:  Running
- thanosQuerier:  Running
- kubeStateMetrics:  Running
- monitoringPlugin:  Running
- openshiftStateMetrics:  Running
- telemeterClient:  Running
- metricsServer:  Running
- nodeExporter:  Running
- prometheusOperatorAdmissionWebhook:  Running


== Persistent Storage Configuration ==

[source, yaml]
----
PVCs in openshift-monitoring namespace:
NAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
alertmanager-main-db-alertmanager-main-0   Bound    pvc-5891c087-769f-426f-a925-be04857ef3eb   10Gi       RWO            gp3-csi        <unset>                 6h48m
alertmanager-main-db-alertmanager-main-1   Bound    pvc-0e7468f7-9013-4781-a378-e7504c88de8b   10Gi       RWO            gp3-csi        <unset>                 6h48m
prometheus-k8s-db-prometheus-k8s-0         Bound    pvc-a34c3fdb-8967-441e-ac6e-3a50559c074b   10Gi       RWO            gp3-csi        <unset>                 6h59m
prometheus-k8s-db-prometheus-k8s-1         Bound    pvc-50ef79db-6124-485a-b50d-591930fb8a4f   10Gi       RWO            gp3-csi        <unset>                 6h59m

Persistent storage is configured in the monitoring config via volumeClaimTemplate
Prometheus PVC found
Alertmanager PVC found
----

== Resource Requests and Limits ==

[source, yaml]
----
Resource requests and/or limits are configured in the monitoring config

Resource configuration for prometheus:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
----

== Node Placement Configuration ==

[source, yaml]
----
Component prometheusOperator has nodeSelector: true, tolerations: true
Component prometheusK8s has nodeSelector: true, tolerations: true
Component alertmanagerMain has nodeSelector: true, tolerations: true
Component thanosQuerier has nodeSelector: true, tolerations: true
Component kubeStateMetrics has nodeSelector: true, tolerations: true
Component monitoringPlugin has nodeSelector: true, tolerations: true
Component openshiftStateMetrics has nodeSelector: true, tolerations: true
Component telemeterClient has nodeSelector: true, tolerations: true
Component metricsServer is not configured in the monitoring config

WARNING: Missing node placement configuration for components: metricsServer

Infrastructure nodes found in the cluster

Monitoring pods are running on infrastructure nodes
----

== Data Retention Configuration ==

[source, yaml]
----
Prometheus data retention is configured

Retention time: retention: 7d
No retention size limit configured
----

== Remote Write Configuration ==

[source, yaml]
----
No remote write configuration found
Consider configuring remote write for long-term metrics storage
----

== Alert Routing Configuration ==

[source, yaml]
----
Alertmanager configuration:
"inhibit_rules":
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = critical"
  "target_matchers":
  - "severity =~ warning|info"
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = warning"
  "target_matchers":
  - "severity = info"
"receivers":
- "name": "Default"
- "name": "Watchdog"
- "name": "Critical"
"route":
  "group_by":
  - "namespace"
  "group_interval": "5m"
  "group_wait": "30s"
  "receiver": "Default"
  "repeat_interval": "12h"
  "routes":
  - "matchers":
    - "alertname = Watchdog"
    "receiver": "Watchdog"
  - "matchers":
    - "severity = critical"
    "receiver": "Critical"

No custom alert receivers found beyond default configuration
Configure alert receivers to ensure notifications are sent to the right teams

No specific alert receiver configurations found
----

== Best Practices Recommendations ==

1. For production clusters, configure persistent storage for monitoring components
2. Set appropriate CPU and memory limits for monitoring components
3. Configure node placement to isolate monitoring components on dedicated nodes
4. Set appropriate retention time and size for Prometheus data
5. Configure remote write for long-term metrics storage
6. Set up alert routing to ensure notifications reach the right people
7. Enable user workload monitoring for application metrics

== Sample Configuration ==

Below is a sample configuration for the cluster-monitoring-config ConfigMap that includes proper resource limits, storage configuration, and node placement:

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
----

Note: The above is a simplified example. You should adjust resource limits, storage size, and other parameters based on your cluster size and workload.

----

**Observation**

Monitoring stack configuration needs improvement: monitoring stack is missing configuration for components: metricsServer, node placement configuration incomplete for components: metricsServer, remote write storage not configured for long-term metrics retention

**Recommendation**

Configure all recommended monitoring stack components for comprehensive monitoring

Configure nodeSelector and tolerations for all monitoring components to place them on infrastructure nodes

Configure remote write storage for long-term metrics retention and historical analysis

Configure alert routing to ensure alerts are properly delivered to the right teams

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/monitoring/index

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Logging Forwarders OPS

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: v1
items:
- apiVersion: observability.openshift.io/v1
  kind: ClusterLogForwarder
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"observability.openshift.io/v1","kind":"ClusterLogForwarder","metadata":{"annotations":{},"name":"instance","namespace":"openshift-logging"},"spec":{"outputs":[{"lokiStack":{"authentication":{"token":{"from":"serviceAccount"}},"target":{"name":"logging-loki","namespace":"openshift-logging"}},"name":"lokistack-out","tls":{"ca":{"configMapName":"openshift-service-ca.crt","key":"service-ca.crt"}},"type":"lokiStack"}],"pipelines":[{"inputRefs":["application","infrastructure"],"name":"infra-app-logs","outputRefs":["lokistack-out"]}],"serviceAccount":{"name":"logging-collector"}}}
    creationTimestamp: "2025-04-14T18:53:57Z"
    generation: 1
    name: instance
    namespace: openshift-logging
    resourceVersion: "434312"
    uid: 8d070534-f15d-4899-b2ae-48290af92d33
  spec:
    managementState: Managed
    outputs:
    - lokiStack:
        authentication:
          token:
            from: serviceAccount
        target:
          name: logging-loki
          namespace: openshift-logging
      name: lokistack-out
      tls:
        ca:
          configMapName: openshift-service-ca.crt
          key: service-ca.crt
      type: lokiStack
    pipelines:
    - inputRefs:
      - application
      - infrastructure
      name: infra-app-logs
      outputRefs:
      - lokistack-out
    serviceAccount:
      name: logging-collector
  status:
    conditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: 'permitted to collect log types: [application infrastructure]'
      reason: ClusterRolesExist
      status: "True"
      type: observability.openshift.io/Authorized
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: ""
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/Valid
    - lastTransitionTime: "2025-04-15T04:41:19Z"
      message: ""
      reason: ReconciliationComplete
      status: "True"
      type: Ready
    inputConditions:
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: input "application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-application
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: input "infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-infrastructure
    outputConditions:
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: output "lokistack-out-application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-application
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: output "lokistack-out-infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-infrastructure
    pipelineConditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: pipeline "infra-app-logs" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: pipeline "infra-app-logs-1" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs-1
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

No external log forwarding is configured for operations logs

**Recommendation**

Configure external forwarding for infrastructure and audit logs

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-external

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Log Forwarding

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: v1
items:
- apiVersion: observability.openshift.io/v1
  kind: ClusterLogForwarder
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"observability.openshift.io/v1","kind":"ClusterLogForwarder","metadata":{"annotations":{},"name":"instance","namespace":"openshift-logging"},"spec":{"outputs":[{"lokiStack":{"authentication":{"token":{"from":"serviceAccount"}},"target":{"name":"logging-loki","namespace":"openshift-logging"}},"name":"lokistack-out","tls":{"ca":{"configMapName":"openshift-service-ca.crt","key":"service-ca.crt"}},"type":"lokiStack"}],"pipelines":[{"inputRefs":["application","infrastructure"],"name":"infra-app-logs","outputRefs":["lokistack-out"]}],"serviceAccount":{"name":"logging-collector"}}}
    creationTimestamp: "2025-04-14T18:53:57Z"
    generation: 1
    name: instance
    namespace: openshift-logging
    resourceVersion: "434312"
    uid: 8d070534-f15d-4899-b2ae-48290af92d33
  spec:
    managementState: Managed
    outputs:
    - lokiStack:
        authentication:
          token:
            from: serviceAccount
        target:
          name: logging-loki
          namespace: openshift-logging
      name: lokistack-out
      tls:
        ca:
          configMapName: openshift-service-ca.crt
          key: service-ca.crt
      type: lokiStack
    pipelines:
    - inputRefs:
      - application
      - infrastructure
      name: infra-app-logs
      outputRefs:
      - lokistack-out
    serviceAccount:
      name: logging-collector
  status:
    conditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: 'permitted to collect log types: [application infrastructure]'
      reason: ClusterRolesExist
      status: "True"
      type: observability.openshift.io/Authorized
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: ""
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/Valid
    - lastTransitionTime: "2025-04-15T04:41:19Z"
      message: ""
      reason: ReconciliationComplete
      status: "True"
      type: Ready
    inputConditions:
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: input "application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-application
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: input "infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-infrastructure
    outputConditions:
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: output "lokistack-out-application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-application
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: output "lokistack-out-infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-infrastructure
    pipelineConditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: pipeline "infra-app-logs" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: pipeline "infra-app-logs-1" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs-1
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

Loki logging is configured but no external forwarding is set up

**Recommendation**

Configure external log forwarding for long-term storage and better log management

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-external

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Logging Installation

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
apiVersion: v1
items:
- apiVersion: observability.openshift.io/v1
  kind: ClusterLogForwarder
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"observability.openshift.io/v1","kind":"ClusterLogForwarder","metadata":{"annotations":{},"name":"instance","namespace":"openshift-logging"},"spec":{"outputs":[{"lokiStack":{"authentication":{"token":{"from":"serviceAccount"}},"target":{"name":"logging-loki","namespace":"openshift-logging"}},"name":"lokistack-out","tls":{"ca":{"configMapName":"openshift-service-ca.crt","key":"service-ca.crt"}},"type":"lokiStack"}],"pipelines":[{"inputRefs":["application","infrastructure"],"name":"infra-app-logs","outputRefs":["lokistack-out"]}],"serviceAccount":{"name":"logging-collector"}}}
    creationTimestamp: "2025-04-14T18:53:57Z"
    generation: 1
    name: instance
    namespace: openshift-logging
    resourceVersion: "434312"
    uid: 8d070534-f15d-4899-b2ae-48290af92d33
  spec:
    managementState: Managed
    outputs:
    - lokiStack:
        authentication:
          token:
            from: serviceAccount
        target:
          name: logging-loki
          namespace: openshift-logging
      name: lokistack-out
      tls:
        ca:
          configMapName: openshift-service-ca.crt
          key: service-ca.crt
      type: lokiStack
    pipelines:
    - inputRefs:
      - application
      - infrastructure
      name: infra-app-logs
      outputRefs:
      - lokistack-out
    serviceAccount:
      name: logging-collector
  status:
    conditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: 'permitted to collect log types: [application infrastructure]'
      reason: ClusterRolesExist
      status: "True"
      type: observability.openshift.io/Authorized
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: ""
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/Valid
    - lastTransitionTime: "2025-04-15T04:41:19Z"
      message: ""
      reason: ReconciliationComplete
      status: "True"
      type: Ready
    inputConditions:
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: input "application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-application
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: input "infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidInput-infrastructure
    outputConditions:
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: output "lokistack-out-application" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-application
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: output "lokistack-out-infrastructure" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidOutput-lokistack-out-infrastructure
    pipelineConditions:
    - lastTransitionTime: "2025-04-14T18:53:57Z"
      message: pipeline "infra-app-logs" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs
    - lastTransitionTime: "2025-04-15T17:19:58Z"
      message: pipeline "infra-app-logs-1" is valid
      reason: ValidationSuccess
      status: "True"
      type: observability.openshift.io/ValidPipeline-infra-app-logs-1
kind: List
metadata:
  resourceVersion: ""


=== Loki Stack Configuration ===
apiVersion: v1
items:
- apiVersion: loki.grafana.com/v1
  kind: LokiStack
  metadata:
    annotations:
      loki.grafana.com/rulesDiscoveredAt: "2025-04-15T13:06:47Z"
    creationTimestamp: "2025-04-14T18:51:50Z"
    generation: 1
    name: lokistack
    namespace: openshift-logging
    resourceVersion: "284969"
    uid: bdbeca6f-5360-47be-abcc-082bcaf90ec9
  spec:
    hashRing:
      type: memberlist
    limits:
      global:
        queries:
          queryTimeout: 3m
    managementState: Managed
    size: 1x.extra-small
    storage:
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
      secret:
        name: logging-loki-s3
        type: s3
    storageClassName: gp3-csi
    tenants:
      mode: openshift-logging
  status:
    components:
      compactor:
        Ready:
        - lokistack-compactor-0
      distributor:
        Ready:
        - lokistack-distributor-756f98f45b-2j5zx
        - lokistack-distributor-756f98f45b-ssvd8
      gateway:
        Ready:
        - lokistack-gateway-5b6f4ccdfb-rg96z
        - lokistack-gateway-5b6f4ccdfb-v54nd
      indexGateway:
        Ready:
        - lokistack-index-gateway-0
        - lokistack-index-gateway-1
      ingester:
        Ready:
        - lokistack-ingester-1
        - lokistack-ingester-0
      querier:
        Ready:
        - lokistack-querier-64b9849496-5j7mt
        - lokistack-querier-64b9849496-tfrp8
      queryFrontend:
        Ready:
        - lokistack-query-frontend-6bffbfbf-5sfk5
        - lokistack-query-frontend-6bffbfbf-8xk5n
      ruler:
        Failed: []
        Pending: []
        Ready: []
        Running: []
    conditions:
    - lastTransitionTime: "2025-04-15T13:07:09Z"
      message: The schema configuration does not contain the most recent schema version
        and needs an update
      reason: StorageNeedsSchemaUpdate
      status: "True"
      type: Warning
    - lastTransitionTime: "2025-04-15T13:07:09Z"
      message: All components ready
      reason: ReadyComponents
      status: "True"
      type: Ready
    - lastTransitionTime: "2025-04-15T13:07:09Z"
      message: All components are running, but some readiness checks are failing
      reason: PendingComponents
      status: "False"
      type: Pending
    storage:
      credentialMode: static
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

OpenShift Logging with Loki is installed and configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Logging Health

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

**Observation**

Loki Logging is healthy

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Logging Component Placement

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

**Observation**

All Loki pods are scheduled on infrastructure nodes

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== OpenShift Logging Storage

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
apiVersion: v1
items:
- apiVersion: loki.grafana.com/v1
  kind: LokiStack
  metadata:
    annotations:
      loki.grafana.com/rulesDiscoveredAt: "2025-04-15T13:06:47Z"
    creationTimestamp: "2025-04-14T18:51:50Z"
    generation: 1
    name: lokistack
    namespace: openshift-logging
    resourceVersion: "284969"
    uid: bdbeca6f-5360-47be-abcc-082bcaf90ec9
  spec:
    hashRing:
      type: memberlist
    limits:
      global:
        queries:
          queryTimeout: 3m
    managementState: Managed
    size: 1x.extra-small
    storage:
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
      secret:
        name: logging-loki-s3
        type: s3
    storageClassName: gp3-csi
    tenants:
      mode: openshift-logging
  status:
    components:
      compactor:
        Ready:
        - lokistack-compactor-0
      distributor:
        Ready:
        - lokistack-distributor-756f98f45b-2j5zx
        - lokistack-distributor-756f98f45b-ssvd8
      gateway:
        Ready:
        - lokistack-gateway-5b6f4ccdfb-rg96z
        - lokistack-gateway-5b6f4ccdfb-v54nd
      indexGateway:
        Ready:
        - lokistack-index-gateway-0
        - lokistack-index-gateway-1
      ingester:
        Ready:
        - lokistack-ingester-1
        - lokistack-ingester-0
      querier:
        Ready:
        - lokistack-querier-64b9849496-5j7mt
        - lokistack-querier-64b9849496-tfrp8
      queryFrontend:
        Ready:
        - lokistack-query-frontend-6bffbfbf-5sfk5
        - lokistack-query-frontend-6bffbfbf-8xk5n
      ruler:
        Failed: []
        Pending: []
        Ready: []
        Running: []
    conditions:
    - lastTransitionTime: "2025-04-15T13:07:09Z"
      message: The schema configuration does not contain the most recent schema version
        and needs an update
      reason: StorageNeedsSchemaUpdate
      status: "True"
      type: Warning
    - lastTransitionTime: "2025-04-15T13:07:09Z"
      message: All components ready
      reason: ReadyComponents
      status: "True"
      type: Ready
    - lastTransitionTime: "2025-04-15T13:07:09Z"
      message: All components are running, but some readiness checks are failing
      reason: PendingComponents
      status: "False"
      type: Pending
    storage:
      credentialMode: static
      schemas:
      - effectiveDate: "2020-10-11"
        version: v11
kind: List
metadata:
  resourceVersion: ""

----

**Observation**

Loki storage schema needs to be updated

**Recommendation**

Update the Loki storage schema to the latest version

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/logging/index

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Service Monitors

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
ServiceMonitors found:

Namespace: user1-argocd, Name: user1-argo
Namespace: user1-argocd, Name: user1-argo-repo-server
Namespace: user1-argocd, Name: user1-argo-server

----

**Observation**

Found 3 ServiceMonitors for application metrics monitoring

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Alerts Forwarding

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
Alertmanager configuration:
"inhibit_rules":
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = critical"
  "target_matchers":
  - "severity =~ warning|info"
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = warning"
  "target_matchers":
  - "severity = info"
"receivers":
- "name": "Default"
- "name": "Watchdog"
- "name": "Critical"
"route":
  "group_by":
  - "namespace"
  "group_interval": "5m"
  "group_wait": "30s"
  "receiver": "Default"
  "repeat_interval": "12h"
  "routes":
  - "matchers":
    - "alertname = Watchdog"
    "receiver": "Watchdog"
  - "matchers":
    - "severity = critical"
    "receiver": "Critical"
----

**Observation**

No external alert forwarding configured

**Recommendation**

Configure external receivers for alerts to ensure notifications are sent to the right teams

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/monitoring/index#sending-notifications-to-external-systems_managing-alerts

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== Monitoring Storage

[cols="^"]
|===
|
{set:cellbgcolor:#00FF00}
No Change
|===

[source, bash]
----
apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"config.yaml":"enableUserWorkload: true\nprometheusOperator:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nnodeExporter:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 150Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\nprometheusK8s:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 3Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  retention: 7d\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 40Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nalertmanagerMain:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 10Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nmonitoringPlugin:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nopenshiftStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\ntelemeterClient:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nk8sPrometheusAdapter:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nthanosQuerier:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nprometheusOperatorAdmissionWebhook:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 100Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"cluster-monitoring-config","namespace":"openshift-monitoring"}}
  creationTimestamp: "2025-04-15T10:31:14Z"
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  resourceVersion: "328571"
  uid: 11f1e0ef-c0da-4948-99d0-4425e5996ebe

----

**Observation**

OpenShift monitoring components have persistent storage configured

**Recommendation**

None

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

== User Workload Monitoring

[cols="^"]
|===
|
{set:cellbgcolor:#FEFE20}
Changes Recommended
|===

[source, bash]
----
== User Workload Monitoring Configuration ==

Cluster Monitoring Config exists:

[source, yaml]
----
apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
      retention: 7d
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 40Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      volumeClaimTemplate:
        spec:
          storageClassName: gp3-csi
          resources:
            requests:
              storage: 10Gi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"config.yaml":"enableUserWorkload: true\nprometheusOperator:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nnodeExporter:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 150Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\nprometheusK8s:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 3Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  retention: 7d\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 40Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nalertmanagerMain:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  volumeClaimTemplate:\n    spec:\n      storageClassName: gp3-csi\n      resources:\n        requests:\n          storage: 10Gi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nmonitoringPlugin:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nopenshiftStateMetrics:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\ntelemeterClient:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nk8sPrometheusAdapter:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nthanosQuerier:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 500Mi\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/infra\n    value: reserved\n    effect: NoExecute\nprometheusOperatorAdmissionWebhook:\n  resources:\n    limits:\n      cpu: 50m\n      memory: 100Mi\n    requests:\n      cpu: 20m\n      memory: 50Mi\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"cluster-monitoring-config","namespace":"openshift-monitoring"}}
  creationTimestamp: "2025-04-15T10:31:14Z"
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  resourceVersion: "328571"
  uid: 11f1e0ef-c0da-4948-99d0-4425e5996ebe

----

User Workload Monitoring enabled in config: true
User Workload Monitoring namespace exists: true

== User Workload Monitoring Components ==

User Workload Monitoring Config exists:

[source, yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: "2025-04-15T10:21:12Z"
  labels:
    app.kubernetes.io/managed-by: cluster-monitoring-operator
    app.kubernetes.io/part-of: openshift-monitoring
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
  resourceVersion: "260740"
  uid: b61d5ceb-3362-4b84-82a1-95b9bd578f87

----

WARNING: No components are explicitly configured in the user-workload-monitoring-config ConfigMap.
All components are using default settings which may not be optimal for production.

WARNING: The following required components are NOT explicitly configured:

- prometheusOperator
- prometheus
- thanosRuler
- alertmanager

These components are running with default settings, which may not be optimal for your environment.
Consider configuring these components explicitly for better control over resources and behavior.

Verifying component status in the cluster:

- prometheusOperator:  Running
- prometheus:  Running
- thanosRuler:  Running
- alertmanager:  Not found or not ready

----

**Observation**

User Workload Monitoring is enabled but missing configuration for components: prometheusOperator, prometheus, thanosRuler, alertmanager

**Recommendation**

Configure all required components for User Workload Monitoring

Refer to https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/monitoring/index#configuring-the-monitoring-stack

*Reference Link(s)*

* https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/

// Reset bgcolor for future tables
[grid=none,frame=none]
|===
|{set:cellbgcolor!}
|===

